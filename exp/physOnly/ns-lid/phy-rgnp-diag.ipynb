{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! If you want to force use of CPU (before import of tensorflow)\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "# os.environ['OMP_NUM_THREADS'] = '4'\n",
    "# os.environ['export OPENBLAS_NUM_THREADS']='4'\n",
    "\n",
    "#! import as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "#! If you want to limit number of threads, the mechine has 32 virtual cores (16 physical ones)\n",
    "print('thread inter = ', tf.config.threading.get_inter_op_parallelism_threads() )\n",
    "print('thread intra = ', tf.config.threading.get_intra_op_parallelism_threads() )\n",
    " \n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "#! If you want to limit the GPU memory usage, the machine has ~24GB. For most applications a few GBs is enough.\n",
    "#! If you max out what you specify progressively up the threshhold.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.set_logical_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=20_000)])\n",
    "\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "except:\n",
    "  # Invalid device or cannot modify logical devices once initialized.\n",
    "  pass\n",
    "\n",
    "#! Will print out what devices you are using\n",
    "tf.config.list_physical_devices()\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mlkernels.tensorflow import Matern32, EQ, Matern12, Matern52\n",
    "\n",
    "\n",
    "from LFM.NeuralNet import NeuralN\n",
    "from LFM.utilities import pshape\n",
    "from LFM.utilities import reparameterize, log_normal_pdf, expHighLowFunc_01, exp_start_end_x_func\n",
    "from LFM.GICNet    import FFLayer, KIPLayer, GICNet\n",
    "\n",
    "pi  = tf.constant(np.pi, tf.float32)\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "dirname='data-lfmDiag-nslid-smallColLonTrain/'\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimU = 3\n",
    "dimZ = 2\n",
    "dimW = 1\n",
    "dimX = 3\n",
    "                         #  x         y         t\n",
    "domain     = tf.constant( [[0., 1.], [0., 1.], [0., 1.]] )\n",
    "\n",
    "nxSamples  = 512\n",
    "# 9^3\n",
    "\n",
    "# currently in dataset                rho,  mu                                  rho, mu  \n",
    "zDist = tfd.Uniform(low=tf.constant([0.8, 0.1], shape=[dimZ]), high=tf.constant([1., 1.], shape=[dimZ]))\n",
    "wDist = tfd.Uniform(low=tf.constant([1.], shape=[dimW]), high=tf.constant([1.], shape=[dimW]))\n",
    "\n",
    "e_bound = 0.\n",
    "\n",
    "def raveled3DDomainGrid(resolution):\n",
    "    x, y, t       = tf.linspace(domain[0][0], domain[0][1], resolution[0]), tf.linspace(domain[1][0], domain[1][1], resolution[1]), \\\n",
    "                    tf.linspace(domain[2][0], domain[2][1], resolution[2])\n",
    "    X, Y, T       = np.meshgrid(x, y, t, indexing='ij')\n",
    "    Xr, Yr, Tr    = tf.reshape(tf.constant(X, dtype=tf.float32), [-1,1]), tf.reshape(tf.constant(Y, dtype=tf.float32), [-1,1]),\\\n",
    "                    tf.reshape(tf.constant(T, dtype=tf.float32), [-1,1])\n",
    "    XYTr          = tf.concat([Xr, Yr, Tr], 1)\n",
    "    return XYTr\n",
    "\n",
    "def sampleDomain(n):\n",
    "    xrand = tf.random.uniform(minval=domain[0][0]+e_bound, maxval=domain[0][1]-e_bound, shape=[n, 1])\n",
    "    yrand = tf.random.uniform(minval=domain[1][0]+e_bound, maxval=domain[1][1]-e_bound, shape=[n, 1])\n",
    "    trand = tf.random.uniform(minval=domain[2][0]+e_bound, maxval=domain[2][1], shape=[n, 1])\n",
    "    return tf.concat([xrand, yrand, trand], 1)\n",
    "\n",
    "print('samples.shape', sampleDomain(10))\n",
    "\n",
    "b      = zDist.sample()\n",
    "xtvect = sampleDomain(nxSamples)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha network\n",
    "\n",
    "dim_DeepO_out  = dimU * 2\n",
    "\n",
    "alphaNet  = NeuralN('alpha', np.array([ 1, 1, dimZ]) )\n",
    "\n",
    "numFreq = 100\n",
    "nhAlpha = 200\n",
    "\n",
    "activ_alpha = 'swish'\n",
    "\n",
    "alphaNet.NN = tf.keras.Sequential([ \n",
    "                                tf.keras.layers.Dense( nhAlpha, input_dim=( numFreq * 2 + dimZ + dimW ), activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( nhAlpha, activation=activ_alpha),\n",
    "                                tf.keras.layers.Dense( dim_DeepO_out ) ])\n",
    "\n",
    "freqsX = tf.linspace(0., 1., numFreq)\n",
    "\n",
    "freqs_x  = expHighLowFunc_01(1., 0.01, freqsX)[None,:]\n",
    "freqs_y  = expHighLowFunc_01(1., 0.01, freqsX)[None,:]\n",
    "freqs_t  = expHighLowFunc_01(1., 0.01, freqsX)[None,:]\n",
    "freqs    = tf.concat([freqs_x, freqs_y, freqs_t], 1)\n",
    "#now they are trainable\n",
    "alphaNet.fLayer = [ FFLayer( freqs, trainable=True ) ]\n",
    "\n",
    "alphaNet.lower_alpha = tf.constant(1e-4)\n",
    "alphaNet.upper_w     = tf.constant(1.)\n",
    "\n",
    "def alphaMap(self, z, w, x):\n",
    "\n",
    "        z_tile  = tf.tile(tf.reshape(z, [1,-1]), [x.shape[0], 1])\n",
    "        w_tile  = tf.tile(tf.reshape(w, [1,-1]), [x.shape[0], 1])\n",
    "        print('x.shape', x.shape)\n",
    "\n",
    "        xFreq   = self.fLayer[0]( x )\n",
    "        x_input = tf.concat([xFreq, z_tile, w_tile], 1)\n",
    "\n",
    "        print('x_input.shape', x_input.shape)\n",
    "        mapped  = self.NN( x_input )\n",
    "        \n",
    "        mean, logvar = mapped[:,:dimU], mapped[:,dimU:2*dimU]\n",
    "        \n",
    "        tf.debugging.assert_all_finite(mean, 'mean not finite')\n",
    "        tf.debugging.assert_all_finite(logvar, 'logvar not finite')\n",
    "\n",
    "        logvar = self.dBoundwalpha_log( logvar )\n",
    "\n",
    "        print('alpha mean.shape = ',   mean.shape)\n",
    "        print('alpha logvar.shape = ', logvar.shape)\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "from types import MethodType\n",
    "alphaNet.call = MethodType(alphaMap, alphaNet)\n",
    "alphaNet(tf.constant(1., shape=[dimZ]), tf.constant(1., shape=[dimW]), sampleDomain(30))\n",
    "alphaNet.compile()\n",
    "alphaNet.NN.summary()\n",
    "alphaNet.summary()\n",
    "\n",
    "# alphaNet.save_weights(dirname + '/alphaModel/alphaModel-lfmDiag')\n",
    "# alphaNet.load_weights(dirname + '/alphaModel/alphaModel-lfmDiag')\n",
    "\n",
    "# del alphaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define beta network\n",
    "\n",
    "\n",
    "dim_trunc_in_beta   = dimW\n",
    "dim_trunc_out_beta  = 200 \n",
    "\n",
    "dim_branch_in_y_beta = 3\n",
    "dim_branch_P_beta    = 30\n",
    "dv_beta              = 33 # number of channels -- filters\n",
    "dim_branch_out_beta  = 200\n",
    "\n",
    "\n",
    "dim_DeepO_in_beta   = dim_trunc_out_beta + dim_branch_out_beta\n",
    "dim_DeepO_out_beta  = dimZ * 2\n",
    "\n",
    "kip_res       = tf.constant([10, 10, 10])\n",
    "grid_kip_beta = raveled3DDomainGrid( kip_res )\n",
    "\n",
    "gicNetBeta = GICNet( grid_kip_beta, kip_res, dim_branch_in_y_beta, dim_branch_P_beta, dimX  )\n",
    "\n",
    "gicNetBeta.P_kLayers   = [ tf.keras.layers.Dense( dim_branch_P_beta, input_dim=( dim_branch_in_y_beta ), activation ='swish'),\n",
    "                        tf.keras.layers.Dense( dim_branch_P_beta, activation ='swish'),\n",
    "                        tf.keras.layers.Dense( dim_branch_P_beta, activation ='swish'),\n",
    "                        tf.keras.layers.Dense( dim_branch_P_beta, activation ='swish') ]\n",
    "                        \n",
    "gicNetBeta.kipLayers   = [ ]\n",
    "ellXs = tf.linspace(0., 1., dim_branch_P_beta)\n",
    "low  = 1e-3\n",
    "high = 1.\n",
    "ellVals = expHighLowFunc_01(low, high, ellXs)\n",
    "for i in range(dim_branch_P_beta):\n",
    "\n",
    "        gicNetBeta.kipLayers.append(KIPLayer(EQ(), ellVals[i]))\n",
    "        print('kip layer {} ell = '.format(i), gicNetBeta.kipLayers[i].ell)\n",
    "\n",
    "\n",
    "gicNetBeta.FFMlayer    = [ ]\n",
    "\n",
    "gicNetBeta.preTrunclayers  = [ \n",
    "                        tf.keras.layers.Dense( dim_trunc_out_beta, input_dim=( dim_trunc_in_beta ), activation ='swish'), \n",
    "                        tf.keras.layers.Dense( dim_trunc_out_beta,                                  activation ='swish'), \n",
    "                        tf.keras.layers.Dense( dim_trunc_out_beta,                                  activation ='swish')\n",
    "                        ]\n",
    "\n",
    "swap_txy_layer = lambda x: tf.transpose(x, [0, 3, 1, 2, 4]) \n",
    "swap_xyt_layer = lambda x: tf.transpose(x, [0, 2, 3, 1, 4]) \n",
    "\n",
    "gicNetBeta.convLayers = [       # 0      1         2         3         4\n",
    "                                #[batch, conv_1 x, conv_2 y, conv_3 t, channel]\n",
    "                                #[batch, batch , conv_2, conv_3, channel]\n",
    "                                #[batch, batch , batch, conv_1, channel]\n",
    "                        # xyt\n",
    "                        tf.keras.layers.Lambda(swap_txy_layer), #txy\n",
    "                        tf.keras.layers.Conv2D( dv_beta, (2,2), strides=(1,1), padding='valid', activation='swish' ),\n",
    "                        tf.keras.layers.Lambda(swap_xyt_layer), # xyt\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (2), strides=(1), padding='valid', activation='swish' ),\n",
    "\n",
    "                        tf.keras.layers.Lambda(swap_txy_layer), #txy\n",
    "                        tf.keras.layers.Conv2D( dv_beta, (2,2), strides=(1,1), padding='valid', activation='swish' ),\n",
    "                        tf.keras.layers.Lambda(swap_xyt_layer), # xyt\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (2), strides=(1), padding='valid', activation='swish' ),\n",
    "\n",
    "                        tf.keras.layers.Lambda(swap_txy_layer), #txy\n",
    "                        tf.keras.layers.Conv2D( dv_beta, (2,2), strides=(1,1), padding='valid', activation='swish' ),\n",
    "                        tf.keras.layers.Lambda(swap_xyt_layer), # xyt\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (2), strides=(1), padding='valid', activation='swish' ),\n",
    "\n",
    "                        tf.keras.layers.Lambda(swap_txy_layer), #txy\n",
    "                        tf.keras.layers.Conv2D( dv_beta, (2,2), strides=(1,1), padding='valid', activation='swish' ),\n",
    "                        tf.keras.layers.Lambda(swap_xyt_layer), # xyt\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (2), strides=(1), padding='valid', activation='swish' ),\n",
    "\n",
    "\n",
    "                        tf.keras.layers.Flatten(  ),\n",
    "                        tf.keras.layers.Dense(1000),\n",
    "                        tf.keras.layers.Dense(dim_branch_out_beta),\n",
    "                        ]\n",
    "                        \n",
    "nn  = tf.keras.Sequential( gicNetBeta.convLayers  )\n",
    "val = tf.random.uniform(shape=[1,kip_res[0], kip_res[1], kip_res[2], dim_branch_P_beta + dimX])\n",
    "nn(val)\n",
    "nn.summary()\n",
    "nn, val = None, None\n",
    "\n",
    "nhBeta       = 400\n",
    "deep0_beta   = tf.keras.layers.Dense( nhBeta, input_dim=( dim_DeepO_in_beta ), activation=tf.keras.activations.swish)\n",
    "deep1_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep2_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep3_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep2_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep3_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deepOut_beta = tf.keras.layers.Dense( dim_DeepO_out_beta )\n",
    "gicNetBeta.DeepOlayers = [ deep0_beta, deep1_beta, deep2_beta, deep3_beta, deepOut_beta ]\n",
    "\n",
    "\n",
    "betaNet    = NeuralN('beta', np.array([ 1, 1, dimZ]) )\n",
    "betaNet.NN = gicNetBeta\n",
    "\n",
    "betaNet.lower_alpha  = tf.constant(1e-4) \n",
    "betaNet.upper_w      = tf.constant(1.)\n",
    "\n",
    "def betaMap(self, u, w, x):\n",
    "\n",
    "        trunc_in  = tf.reshape(w, [-1,dimW])\n",
    "        branch_in = tf.concat([x, u], 1)\n",
    "\n",
    "        mapped = self.NN( branch_in, trunc_in )\n",
    "        print('mapped.shape = ', mapped.shape)\n",
    "\n",
    "        mean, logvar = mapped[:,:dimZ], mapped[:,dimZ:2*dimZ]\n",
    "\n",
    "        tf.debugging.assert_all_finite(mean, 'mean not finite')\n",
    "        tf.debugging.assert_all_finite(logvar, 'logvar not finite')\n",
    "\n",
    "        logvar = self.dBoundwalpha_log( logvar )\n",
    "        print('beta mean.shape = ', mean.shape)\n",
    "        print('beta logvar.shape = ', logvar.shape)\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "from types import MethodType\n",
    "betaNet.call = MethodType(betaMap, betaNet)\n",
    "\n",
    "mean_alpha, logvar_alpha = alphaNet(tf.constant(1., shape=[dimZ]), tf.constant(1., shape=[dimW]), xtvect)\n",
    "print(mean_alpha.shape)\n",
    "\n",
    "mean_beta, logvar_beta = betaNet( mean_alpha, tf.constant(1., shape=[dimW]), xtvect )\n",
    "betaNet.compile( )\n",
    "betaNet.summary( )\n",
    "betaNet.NN.summary( )\n",
    "\n",
    "\n",
    "\n",
    "# betaNet.save_weights(dirname + '/betaModel/betaModel-lfmDiag')\n",
    "# betaNet.load_weights(dirname + '/betaModel/betaModel-lfmDiag')\n",
    "\n",
    "# del betaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def D_square(x, y, t):\n",
    "    return tf.sin(pi * x) * tf.sin(pi * y) * t\n",
    "\n",
    "power_n = 6\n",
    "def base_u(x, y, t, w):\n",
    "    return w * ( 1. - (2*x-1)**power_n ) * y * t\n",
    "\n",
    "def D_p(x, y, t):\n",
    "    return x + y - e_bound * 2.\n",
    "\n",
    "\n",
    "alphaNet.epsilon_r = tf.Variable(0.01, trainable=False)\n",
    "# epsilon_r = tf.Variable(1e-2, trainable=False)\n",
    "\n",
    "epsiBij = tfp.bijectors.SoftClip(low=1e-5, high=1.)\n",
    "\n",
    "def residualFunction( z, w, xyt ):\n",
    "    \n",
    "    x,y, t = xyt[:, 0:1], xyt[:, 1:2], xyt[:, 2:3]\n",
    "    print('x.shape', x.shape)\n",
    "    print('y.shape', y.shape)\n",
    "    print('t.shape', t.shape)\n",
    "\n",
    "    # z[0] = rho, z[1] = mu\n",
    "    z = tf.reshape(z, [-1])\n",
    "    w = tf.reshape(w, [ ])\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t1:\n",
    "        t1.watch( x )\n",
    "        t1.watch( y )\n",
    "\n",
    "        with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t2:\n",
    "            t2.watch( x )\n",
    "            t2.watch( y )\n",
    "            t2.watch( t )\n",
    "\n",
    "            mean_alpha, logvar_alpha = alphaNet( z, w, tf.concat([x,y,t], 1) )\n",
    "\n",
    "            mean_alpha_stack   = tf.reshape(mean_alpha,   [-1,1])\n",
    "            logvar_alpha_stack = tf.reshape(logvar_alpha, [-1,1])\n",
    "\n",
    "            uvp_untrans = tf.reshape(reparameterize(mean_alpha_stack, logvar_alpha_stack), [-1, dimU])\n",
    "\n",
    "            u_untrans, v_untrans, p_untrans = uvp_untrans[:, 0:1], uvp_untrans[:, 1:2], uvp_untrans[:, 2:3]\n",
    "\n",
    "            u = base_u(x, y, t, w) + D_square(x, y, t) * u_untrans\n",
    "            v =                      D_square(x, y, t) * v_untrans\n",
    "            p =                      D_p(x, y, t)      * p_untrans\n",
    "            \n",
    "        u_x = tf.reshape(t2.gradient(u, x), [-1,1])\n",
    "        u_y = tf.reshape(t2.gradient(u, y), [-1,1])\n",
    "        u_t = tf.reshape(t2.gradient(u, t), [-1,1])\n",
    "\n",
    "        v_x = tf.reshape(t2.gradient(v, x), [-1,1])\n",
    "        v_y = tf.reshape(t2.gradient(v, y), [-1,1])\n",
    "        v_t = tf.reshape(t2.gradient(v, t), [-1,1])\n",
    "\n",
    "        p_x = tf.reshape(t2.gradient(p, x), [-1,1])\n",
    "        p_y = tf.reshape(t2.gradient(p, y), [-1,1])\n",
    "\n",
    "    u_xx = tf.reshape(t1.gradient(u_x, x), [-1,1])\n",
    "    u_yy = tf.reshape(t1.gradient(u_y, y), [-1,1])\n",
    "\n",
    "    v_xx = tf.reshape(t1.gradient(v_x, x), [-1,1])\n",
    "    v_yy = tf.reshape(t1.gradient(v_y, y), [-1,1])\n",
    "    \n",
    "    rho, mu = tf.squeeze(z[0]),  tf.squeeze(z[1])\n",
    "    res_1 = rho * u_t + rho * ( u * u_x + v * u_y ) + p_x - mu * ( u_xx + u_yy )\n",
    "    res_2 = rho * v_t + rho * ( u * v_x + v * v_y ) + p_y - mu * ( v_xx + v_yy )\n",
    "    res_3 = u_x + v_y\n",
    "\n",
    "    res   = tf.concat([res_1, res_2, res_3 * 10.], 0) \n",
    "    \n",
    "    tf.debugging.assert_all_finite(res, 'res not finite')\n",
    "    \n",
    "    print(\"residualBody.shape\", res.shape)\n",
    "    print('u.shape', u.shape)\n",
    "    print('mean_alpha.shape', mean_alpha.shape)\n",
    "    print('logvar_alpha.shape', logvar_alpha.shape )\n",
    "        \n",
    "    logP_alpha_u_Z = log_normal_pdf(tf.reshape(uvp_untrans, [-1,1]), mean_alpha_stack, logvar_alpha_stack, raxis=0)\n",
    "\n",
    "    return uvp_untrans, res, logP_alpha_u_Z\n",
    "\n",
    "\n",
    "def ELBO( zw ):\n",
    "\n",
    "    z,w    = zw[:dimZ], zw[dimZ:dimZ+dimW]\n",
    "    xtrand = sampleDomain(nxSamples)\n",
    "\n",
    "    print(\"ELBO xrand.shape\", xtrand.shape)\n",
    "    print('ELBO z.shape', z.shape)\n",
    "    print('ELBO w.shape', w.shape)\n",
    "\n",
    "    uvp_untrans, residual, logP_alpha_u_Z = residualFunction( z, w, xtrand )\n",
    "\n",
    "    logVarEpsVect  = tf.repeat(2.*tf.math.log(alphaNet.epsilon_r), residual.shape[0] )[:, None]\n",
    "\n",
    "    logP_r   = log_normal_pdf( residual, tf.constant(0., shape=residual.shape), logVarEpsVect, raxis=0 )\n",
    "    \n",
    "    mean_beta, logvar_beta = betaNet( tf.reshape(uvp_untrans, [-1,dimU]), w, xtrand )\n",
    "\n",
    "    print('mean_beta.shape', mean_beta.shape)\n",
    "    print('mean_beta.shape', logvar_beta.shape)\n",
    "\n",
    "    logP_beta_Z_u          = log_normal_pdf( tf.reshape(z, [1,-1]), mean_beta, logvar_beta, raxis=1 ) #this is correct dimension\n",
    "\n",
    "    print('logP_alpha_u_Z.shape', logP_alpha_u_Z.shape)\n",
    "    print('logP_beta_Z_u.shape', logP_beta_Z_u.shape)\n",
    "    print('logP_r.shape', logP_r.shape)\n",
    "\n",
    "    tf.debugging.assert_all_finite(logP_r, 'logP_r not finite')\n",
    "    tf.debugging.assert_all_finite(logP_beta_Z_u, 'logP_beta_Z_u not finite')\n",
    "    tf.debugging.assert_all_finite(logP_alpha_u_Z, 'logP_alpha_u_Z not finite')\n",
    "\n",
    "    elbo = tf.reduce_mean( logP_r + logP_beta_Z_u -  logP_alpha_u_Z )\n",
    "\n",
    "    return elbo\n",
    "\n",
    "zs =  zDist.sample( )\n",
    "ws =  wDist.sample( )\n",
    "\n",
    "ELBO( tf.concat([zs, ws], 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainableVars   = [ alphaNet.trainable_variables, betaNet.trainable_variables]\n",
    "\n",
    "@tf.function\n",
    "def train_step(optimizer):\n",
    "\n",
    "    zs =  zDist.sample( batchSize )\n",
    "    ws =  wDist.sample( batchSize )\n",
    "\n",
    "    print('zs.shape', zs.shape)\n",
    "    print('ws.shape', ws.shape)\n",
    "\n",
    "    zws = tf.concat([zs, ws], 1)\n",
    "\n",
    "    print('zwxs.shape', zws.shape)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        losses = tf.vectorized_map(ELBO, zws, fallback_to_while_loop=False)\n",
    "    \n",
    "        loss   = - tf.reduce_mean(losses)\n",
    "        tf.debugging.assert_all_finite(loss, 'loss not finite, {}'.format(losses))\n",
    "        \n",
    "    for vars in trainableVars:\n",
    "        gradients = tape.gradient(loss, vars)\n",
    "        optimizer.apply_gradients(zip(gradients, vars))\n",
    "\n",
    "    return loss\n",
    "\n",
    "batchSize = 5\n",
    "\n",
    "num_iterations   = tf.constant(1_000_000)\n",
    "\n",
    "\n",
    "intervalELBOsave = 100\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    1e-3, int(num_iterations/batchSize/5), 0.5, staircase=True, name=None\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam( lr )\n",
    "\n",
    "\n",
    "elboAll     = []\n",
    "epsAll      = []\n",
    "startTrainingPDDLVM = time.time()\n",
    "for iteration in tf.range(0, int(num_iterations/batchSize) ):\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss = train_step(optimizer)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if iteration % intervalELBOsave == 0:\n",
    "        epsAll.append( alphaNet.epsilon_r.numpy() )\n",
    "        elboAll.append( -loss ) \n",
    "        print('Epoch: {}, Test set ELBO: {:.2f}, epsilon_r {:.4f}, time elapse for current epoch: {:.4f}'\n",
    "        .format(iteration, -loss, alphaNet.epsilon_r.numpy(), end_time - start_time))\n",
    "\n",
    "endTrainingPDDLVM = time.time()\n",
    "totalTimeTrainingPDLVM = (endTrainingPDDLVM - startTrainingPDDLVM)/60.\n",
    "\n",
    "print('totalTimeTrainingPDLVM = {:.2f}m'.format( totalTimeTrainingPDLVM ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveWeights       = False\n",
    "overPowerSaveConf = False\n",
    "\n",
    "loadWeights = True\n",
    "\n",
    "model_name = '0_8-1--0_1-1--1-1--phylvmdiag/'\n",
    "\n",
    "if saveWeights == True:\n",
    "    saveWeightsConf = 'n'\n",
    "    if overPowerSaveConf == False:\n",
    "        saveWeightsConf = input('you want to save/overwrite weights? Y/n')\n",
    "    if saveWeightsConf == 'Y' or overPowerSaveConf == True:\n",
    "        alphaNet.save_weights(dirname+model_name+'alphaModel/checkpoints/my_checkpoint')\n",
    "        betaNet.save_weights(dirname+model_name+'betaNet/checkpoints/my_checkpoint')\n",
    "        print('weights have been saved')\n",
    "\n",
    "if loadWeights == True:\n",
    "    loadWeightsConf = input('you want to load weights? Y/n')\n",
    "    if loadWeightsConf == 'Y':\n",
    "        alphaNet.load_weights(dirname+model_name+'alphaModel/checkpoints/my_checkpoint')\n",
    "        betaNet.load_weights(dirname+model_name+'betaNet/checkpoints/my_checkpoint')\n",
    "        print('weights have been loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.popen('cp ./phy-lvm-diag.ipynb {}phy-lvm-diag.ipynb'.format(dirname))\n",
    "\n",
    "savefile_python = True\n",
    "if savefile_python == True:\n",
    "    filename = 'phy-lvm-diag-smallColLongTrain-1_2'\n",
    "    os.popen('ipynb-py-convert ./{0}.ipynb {1}{2}{0}.py'.format(filename, dirname, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebloAll = tf.stack(elboAll)\n",
    "LOSS    = np.array(ebloAll)\n",
    "plt.plot(range(LOSS.shape[0]), LOSS )\n",
    "plt.show()\n",
    "print(np.isnan(LOSS).mean())\n",
    "plt.plot(range(LOSS.shape[0]), np.log(-LOSS) )\n",
    "plt.show()\n",
    "stack_epsAll = tf.stack(epsAll)\n",
    "plt.plot(range( len(epsAll)), stack_epsAll )\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(dirname  + 'ebloAll.dat' , ebloAll )\n",
    "np.savetxt(dirname  + 'epsAll.dat'  , stack_epsAll )\n",
    "print('totalTimeTrainingPDLVM = {:.2f}m'.format( totalTimeTrainingPDLVM ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [1., 1.]\n",
    "w = [1.]\n",
    "print('plotting')\n",
    "resoTEST = [100, 100, 10]\n",
    "pt = 9\n",
    "X, Y, T = np.meshgrid( tf.linspace(0., 1., resoTEST[0]), tf.linspace(0., 1., resoTEST[1]), tf.linspace(0., 1., resoTEST[2]), indexing='xy')\n",
    "\n",
    "xr, yr, tr = tf.reshape(X, [-1,1]), tf.reshape(Y, [-1,1]), tf.reshape(T, [-1,1])\n",
    "\n",
    "mean_alpha, logvar_alpha = alphaNet( z, w, tf.concat([xr, yr, tr], 1) )\n",
    "\n",
    "mean_alpha_stack   = tf.reshape(mean_alpha,   [-1,1])\n",
    "logvar_alpha_stack = tf.reshape(logvar_alpha, [-1,1])\n",
    "\n",
    "uvp_untrans = mean_alpha\n",
    "\n",
    "\n",
    "u_untrans, v_untrans, p_untrans          = uvp_untrans[:, 0:1], uvp_untrans[:, 1:2], uvp_untrans[:, 2:3]\n",
    "u_untrans_lv, v_untrans_lv, p_untrans_lv = logvar_alpha[:, 0:1], logvar_alpha[:, 1:2], logvar_alpha[:, 2:3]\n",
    "\n",
    "u_vec = base_u(xr, yr, tr, w) + D_square(xr, yr, tr) * u_untrans\n",
    "v_vec =                         D_square(xr, yr, tr) * v_untrans\n",
    "p_vec =                         D_p(xr, yr, tr)      * p_untrans\n",
    "\n",
    "P_vec = tf.reshape(p_vec, resoTEST)\n",
    "U_vec = tf.reshape(u_vec, resoTEST)\n",
    "V_vec = tf.reshape(v_vec, resoTEST)\n",
    "\n",
    "\n",
    "u_vec_std =  tf.sqrt( D_square(xr, yr, tr)**2. * tf.exp( u_untrans_lv ) )\n",
    "v_vec_std =  tf.sqrt( D_square(xr, yr, tr)**2. * tf.exp( v_untrans_lv ) )\n",
    "p_vec_std =  tf.sqrt( D_p(xr, yr, tr)     **2. * tf.exp( p_untrans_lv ) )\n",
    "\n",
    "P_vec_std = tf.reshape(p_vec_std, resoTEST)\n",
    "U_vec_std = tf.reshape(u_vec_std, resoTEST)\n",
    "V_vec_std = tf.reshape(v_vec_std, resoTEST)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "########### plot slns ###########\n",
    "mag_vec = np.array( tf.linalg.norm(tf.concat([U_vec[...,pt:pt+1], V_vec[...,pt:pt+1]],-1), axis=-1))\n",
    "\n",
    "strm  = ax.streamplot(X[...,pt], Y[...,pt], U_vec[...,pt], V_vec[...,pt], color=mag_vec, density=8,cmap = plt.cm.seismic,\n",
    "                        broken_streamlines=False, linewidth=0.1, arrowsize=0.)  \n",
    "# strm  = ax.quiver(X, Y,U_vec, V_vec)\n",
    "# ax.contourf(X,Y, P_vec, 50)\n",
    "fig.colorbar(strm.lines, ax=ax)\n",
    "ax.set(xlabel=r'$x$', ylabel=r'$y$')\n",
    "ax.set_xlim(0., 1.)\n",
    "ax.set_ylim(0., 1.)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'strlines.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.contourf(X[...,pt],Y[...,pt], P_vec[...,pt], 50)\n",
    "plt.colorbar()\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$t$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'P.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.contourf(X[...,pt],Y[...,pt], U_vec[...,pt], 50)\n",
    "plt.colorbar()\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$t$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'U.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.contourf(X[...,pt],Y[...,pt], V_vec[...,pt], 50)\n",
    "plt.colorbar()\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$t$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'V.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "########### plot stddev ###########\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mag_vec_std = np.array( tf.linalg.norm(tf.concat([U_vec_std[...,pt:pt+1], V_vec_std[...,pt:pt+1]],-1), axis=-1))\n",
    "\n",
    "strm  = ax.streamplot(X[...,pt], Y[...,pt], U_vec_std[...,pt], V_vec_std[...,pt], color=mag_vec, density=8,cmap = plt.cm.seismic,\n",
    "                        broken_streamlines=False, linewidth=0.1, arrowsize=0.)  \n",
    "# strm  = ax.quiver(X, Y,U_vec, V_vec)\n",
    "# ax.contourf(X,Y, P_vec, 50)\n",
    "# fig.colorbar(strm.lines, ax=ax)\n",
    "# ax.set(xlabel=r'$x$', ylabel=r'$y$')\n",
    "# ax.set_xlim(0., 1.)\n",
    "# ax.set_ylim(0., 1.)\n",
    "# plt.ylabel(r'$t$')\n",
    "# plt.xlabel(r'$x$')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(dirname + 'strlines_std.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "plt.contourf(X[...,pt],Y[...,pt], P_vec_std[...,pt], 50)\n",
    "plt.colorbar()\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$t$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'Pressure_std.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.contourf(X[...,pt],Y[...,pt], U_vec_std[...,pt], 50)\n",
    "plt.colorbar()\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$t$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'U_std.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.contourf(X[...,pt],Y[...,pt], V_vec_std[...,pt], 50)\n",
    "plt.colorbar()\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$t$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'V_std.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "mean_beta, logvar_beta = betaNet( tf.reshape(uvp_untrans, [-1,dimU]), w[0], tf.concat([xr, yr, tr], 1) ) \n",
    "\n",
    "print('mean_beta = ', mean_beta)\n",
    "print('std_beta = ', tf.exp(logvar_beta * 0.5) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirnameGT = '../../../fenics/NSLid/data-groundTruth-nslid_100_0_8-1--0_1-1--1-1'\n",
    "\n",
    "# saved i, T, X, Y\n",
    "# want  i, X, Y, T\n",
    "P_vec_all = tf.constant( np.load(dirnameGT+'/'+'SolutionsPGT.npy'), dtype=tf.float32)\n",
    "U_vec_all = tf.constant( np.load(dirnameGT+'/'+'SolutionsUGT.npy'), dtype=tf.float32)\n",
    "V_vec_all = tf.constant( np.load(dirnameGT+'/'+'SolutionsVGT.npy'), dtype=tf.float32)\n",
    "\n",
    "pad       = 1\n",
    "padt      = 1\n",
    "P_vec_all = tf.transpose(P_vec_all, [0, 2, 3, 1])[:, pad:-pad, pad:-pad, padt:]\n",
    "U_vec_all = tf.transpose(U_vec_all, [0, 2, 3, 1])[:, pad:-pad, pad:-pad, padt:]\n",
    "V_vec_all = tf.transpose(V_vec_all, [0, 2, 3, 1])[:, pad:-pad, pad:-pad, padt:]\n",
    "\n",
    "allZ      = tf.constant( np.loadtxt(dirnameGT+'/'+'allZ.dat' )    , dtype=tf.float32)\n",
    "allW      = tf.constant( np.loadtxt(dirnameGT+'/'+ 'allW.dat')    , dtype=tf.float32)\n",
    "fenics_XY = tf.constant( np.load(dirnameGT+'/'+'FenicsMeshXY.npy'), dtype=tf.float32)[:, pad:-pad, pad:-pad]\n",
    "fenics_T  = tf.constant( np.load(dirnameGT+'/'+'FenicsMeshT.npy') , dtype=tf.float32)[padt:]\n",
    "\n",
    "\n",
    "print('P_vec_all.shape', P_vec_all.shape)\n",
    "print('U_vec_all.shape', U_vec_all.shape)\n",
    "print('V_vec_all.shape', V_vec_all.shape)\n",
    "\n",
    "print('allZ     .shape', allZ     .shape)\n",
    "print('allW     .shape', allW     .shape)\n",
    "print('fenics_XY.shape', fenics_XY.shape)\n",
    "print('fenics_T .shape', fenics_T .shape)\n",
    "\n",
    "nx = fenics_XY.shape[1]\n",
    "nt = fenics_XY.shape[2]\n",
    "\n",
    "saveDataList = True\n",
    "\n",
    "if saveDataList:\n",
    "\n",
    "     allresAbsMean = []\n",
    "\n",
    "     allSampleZs   = []\n",
    "     allMeanZs     = []\n",
    "     allStdZs      = []\n",
    "\n",
    "     allUSamples = []\n",
    "     allUMeans   = []\n",
    "     allUStd     = []\n",
    "        \n",
    "     allVSamples = []\n",
    "     allVMeans   = []\n",
    "     allVStd     = []\n",
    "\n",
    "     allZNSE       = []\n",
    "     allUNSE       = []\n",
    "     allVNSE       = []\n",
    "\n",
    "\n",
    "     Pin2SigK = []\n",
    "     Pin2SigU = []\n",
    "     Pin2SigV = []\n",
    "\n",
    "# XYT 3, 50,50,20\n",
    "# XY  2, 50, 50\n",
    "# T   20\n",
    "fenics_XYtile = tf.tile(fenics_XY[..., None], [1,1,1,fenics_T.shape[0]])\n",
    "fenics_Ttile  = tf.tile(fenics_T[None, None, None, :], [1, fenics_XY.shape[1], fenics_XY.shape[2], 1])\n",
    "fenics_XYT    = tf.concat([fenics_XYtile, fenics_Ttile], 0)\n",
    "print('fenicsXYT.shape', fenics_XYT.shape)\n",
    "\n",
    "pt    = fenics_XYT.shape[3]-1\n",
    "print('pt = ', pt)\n",
    "plt.contourf(fenics_XYT[0,...,pt], fenics_XYT[1,...,pt], U_vec_all[0,...,pt], 50)\n",
    "plt.colorbar( )\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(r'$y$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "xr, yr, tr   = tf.reshape(fenics_XYT[0], [-1,1]), tf.reshape(fenics_XYT[1], [-1,1]), tf.reshape(fenics_XYT[2], [-1,1])\n",
    "xyttest_rav  = tf.concat([xr, yr, tr], 1)\n",
    "tf_residualFunction = residualFunction #\n",
    "tf_alphaNet         = alphaNet         #\n",
    "tf_betaNet          = betaNet          #\n",
    "\n",
    "\n",
    "PLOT        = False\n",
    "savePlots   = False\n",
    "BenchParams = False\n",
    "writeToFile = False\n",
    "\n",
    "for i in range(U_vec_all.shape[0]):\n",
    "\n",
    "\n",
    "     zTrue = tf.reshape( allZ[i], [-1, 1] )\n",
    "     wTrue = tf.reshape( allW[i], [ ] )\n",
    "     uTrue = tf.reshape( U_vec_all[i], [-1, 1] )\n",
    "     vTrue = tf.reshape( V_vec_all[i], [-1, 1] )\n",
    "     pTrue = tf.reshape( P_vec_all[i], [-1, 1] )\n",
    "\n",
    "    \n",
    "     if BenchParams:\n",
    "          zTrue = tf.constant([1., 0.1])\n",
    "          wTrue = tf.constant([1.])\n",
    "\n",
    "     print('zTrue = ', zTrue)\n",
    "     print('wTrue = ', wTrue)\n",
    "     \n",
    "     mean_alpha, logvar_alpha = tf_alphaNet( zTrue, wTrue, xyttest_rav )\n",
    "     mean_alpha_stack   = tf.reshape(mean_alpha,   [-1,1])\n",
    "     logvar_alpha_stack = tf.reshape(logvar_alpha, [-1,1])\n",
    " \n",
    "#      uvp_untrans = tf.reshape(reparameterize(mean_alpha_stack, logvar_alpha_stack), [-1, dimU])\n",
    "     uvp_untrans = mean_alpha\n",
    "     u_untrans, v_untrans, p_untrans = uvp_untrans[:, 0:1], uvp_untrans[:, 1:2], uvp_untrans[:, 2:3]\n",
    "\n",
    "     u_vec = base_u(xr, yr, tr, wTrue) + D_square(xr, yr, tr) * u_untrans\n",
    "     v_vec =                             D_square(xr, yr, tr) * v_untrans\n",
    "     p_vec =                             D_p(xr, yr, tr)      * p_untrans\n",
    "\n",
    "     if PLOT == True:\n",
    "          U_vec = tf.reshape(u_vec, fenics_XYT[0].shape)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], u_vec[:,0], 50)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], uTrue[:,0], 50)\n",
    "          plt.contourf(fenics_XYT[0,...,pt],fenics_XYT[1,...,pt], U_vec[...,pt], 50)\n",
    "          plt.colorbar()\n",
    "          plt.xlim(0., 1.)\n",
    "          plt.ylim(0., 1.)\n",
    "          plt.ylabel(r'$t$')\n",
    "          plt.xlabel(r'$x$')\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "          plt.close()\n",
    "\n",
    "          UTrue = tf.reshape(uTrue, fenics_XYT[0].shape)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], u_vec[:,0], 50)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], uTrue[:,0], 50)\n",
    "          plt.contourf(fenics_XYT[0,...,pt],fenics_XYT[1,...,pt], UTrue[...,pt], 50)\n",
    "          plt.colorbar()\n",
    "          plt.xlim(0., 1.)\n",
    "          plt.ylim(0., 1.)\n",
    "          plt.ylabel(r'$t$')\n",
    "          plt.xlabel(r'$x$')\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "          plt.close()\n",
    "\n",
    "          V_vec = tf.reshape(v_vec, fenics_XYT[0].shape)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], u_vec[:,0], 50)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], uTrue[:,0], 50)\n",
    "          plt.contourf(fenics_XYT[0,...,pt],fenics_XYT[1,...,pt], V_vec[...,pt], 50)\n",
    "          plt.colorbar()\n",
    "          plt.xlim(0., 1.)\n",
    "          plt.ylim(0., 1.)\n",
    "          plt.ylabel(r'$t$')\n",
    "          plt.xlabel(r'$x$')\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "          plt.close()\n",
    "\n",
    "          VTrue = tf.reshape(vTrue, fenics_XYT[0].shape)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], u_vec[:,0], 50)\n",
    "          # plt.tricontourf(xr[:,0], yr[:,0], uTrue[:,0], 50)\n",
    "          plt.contourf(fenics_XYT[0,...,pt],fenics_XYT[1,...,pt], VTrue[...,pt], 50)\n",
    "          plt.colorbar()\n",
    "          plt.xlim(0., 1.)\n",
    "          plt.ylim(0., 1.)\n",
    "          plt.ylabel(r'$t$')\n",
    "          plt.xlabel(r'$x$')\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "          plt.close()\n",
    "\n",
    "     u_untrans_lv, v_untrans_lv, p_untrans_lv = logvar_alpha[:, 0:1], logvar_alpha[:, 1:2], logvar_alpha[:, 2:3]\n",
    "\n",
    "     u_vec_std = tf.sqrt( D_square(xr, yr, tr) ** 2. * tf.exp( u_untrans_lv ) )\n",
    "     v_vec_std = tf.sqrt( D_square(xr, yr, tr) ** 2. * tf.exp( v_untrans_lv ) )\n",
    "     p_vec_std = tf.sqrt( D_p(xr, yr, tr)      ** 2. * tf.exp( p_untrans_lv ) )\n",
    "     \n",
    "     p_true_unstranf        = (pTrue ) / ( D_p(xr, yr, tr) )\n",
    "     u_true_unstranf        = (uTrue - base_u(xr, yr, tr, wTrue)) / ( D_square(xr, yr, tr) )\n",
    "     v_true_unstranf        = (vTrue ) / ( D_square(xr, yr, tr) )\n",
    "     uvp_true_untransf      = tf.concat([u_true_unstranf, v_true_unstranf, p_true_unstranf], 1)\n",
    "     tf.debugging.assert_all_finite(uvp_true_untransf, 'uvp_untrsanf not finite')\n",
    "     mean_beta, logvar_beta = tf_betaNet( uvp_true_untransf, wTrue,  xyttest_rav )\n",
    "\n",
    "     mean_beta, logvar_beta = tf.squeeze( mean_beta), tf.squeeze( logvar_beta )\n",
    "     sampleZ  = reparameterize( mean_beta, logvar_beta )\n",
    "     std_beta = tf.exp( 0.5 * logvar_beta)\n",
    "\n",
    "     if saveDataList:\n",
    "\n",
    "          allSampleZs.append(tf.squeeze(sampleZ))\n",
    "          allMeanZs.append(tf.squeeze(mean_beta))\n",
    "          allStdZs.append(std_beta)\n",
    "\n",
    "          allUSamples.append(tf.squeeze(u_vec))\n",
    "          allVSamples.append(tf.squeeze(v_vec))\n",
    "        \n",
    "          allUStd.append(tf.squeeze(u_vec_std))\n",
    "          allVStd.append(tf.squeeze(v_vec_std))\n",
    "          \n",
    "          allZNSE.append( ( tf.linalg.norm((tf.squeeze(mean_beta) - tf.squeeze(zTrue)) ) / tf.linalg.norm(tf.squeeze(zTrue)) )**2. )\n",
    "\n",
    "          allUNSE.append( ( tf.linalg.norm((tf.squeeze(u_vec) - tf.squeeze(uTrue)) ) / tf.linalg.norm(tf.squeeze(uTrue)) )**2. )\n",
    "          allVNSE.append( ( tf.linalg.norm((tf.squeeze(v_vec) - tf.squeeze(vTrue)) ) / tf.linalg.norm(tf.squeeze(vTrue)) )**2. )\n",
    "\n",
    "          Pin2SigK.append( tf.reduce_sum(tf.where(tf.math.logical_and( \n",
    "               tf.squeeze(zTrue) >  tf.squeeze(mean_beta-2*std_beta), tf.squeeze(zTrue) <  tf.squeeze(mean_beta+2*std_beta)\n",
    "               ), 1, 0)) /  tf.squeeze(zTrue).shape[0] )\n",
    "\n",
    "          Pin2SigU.append( tf.reduce_sum(tf.where(tf.math.logical_and(\n",
    "               tf.squeeze(uTrue) > tf.squeeze(u_vec-2*u_vec_std), tf.squeeze(uTrue) < tf.squeeze(u_vec+2*u_vec_std)\n",
    "               ), 1, 0)) / tf.squeeze(uTrue).shape[0]  )\n",
    "        \n",
    "          Pin2SigV.append( tf.reduce_sum(tf.where(tf.math.logical_and(\n",
    "               tf.squeeze(vTrue) > tf.squeeze(v_vec-2*v_vec_std), tf.squeeze(vTrue) < tf.squeeze(v_vec+2*v_vec_std)\n",
    "               ), 1, 0)) / tf.squeeze(vTrue).shape[0]  )\n",
    "\n",
    "\n",
    "print('mean allZNSE = ', tf.reduce_mean( tf.stack(allZNSE) ) )\n",
    "print('mean allUNSE = ', tf.reduce_mean( tf.stack(allUNSE) ) )\n",
    "print('mean allVNSE = ', tf.reduce_mean( tf.stack(allVNSE) ) )\n",
    "\n",
    "print('sttdev allZNSE = ', tf.math.reduce_std( tf.stack(allZNSE) ) )\n",
    "print('sttdev allUNSE = ', tf.math.reduce_std( tf.stack(allUNSE) ) )\n",
    "print('sttdev allVNSE = ', tf.math.reduce_std( tf.stack(allVNSE) ) )\n",
    "\n",
    "\n",
    "# print('median allZNSE = ', tfp.stats.percentile( allZNSE, 50 ) )\n",
    "\n",
    "print('Pin2SigK = ', tf.reduce_mean(tf.stack(Pin2SigK)))\n",
    "print('Pin2SigU = ', tf.reduce_mean(tf.stack(Pin2SigU)))\n",
    "print('Pin2SigV = ', tf.reduce_mean(tf.stack(Pin2SigV)))\n",
    "\n",
    "# print('allUStd = ', tf.reduce_mean(tf.stack(allUStd)))\n",
    "\n",
    "if writeToFile:\n",
    "     with  open(dirname+\"allInfoRun.txt\", \"w\") as f:\n",
    "          f.write('mean allZNSE = {}\\n'.format(tf.reduce_mean( tf.stack(allZNSE) )) )\n",
    "          f.write('mean allUNSE = {}\\n'.format(tf.reduce_mean( tf.stack(allUNSE) )) )\n",
    "          f.write('mean allVNSE = {}\\n'.format(tf.reduce_mean( tf.stack(allVNSE) )) )\n",
    "          f.write('sttdev allZNSE = {}\\n'.format(tf.math.reduce_std( tf.stack(allZNSE) )) )\n",
    "          f.write('sttdev allUNSE = {}\\n'.format(tf.math.reduce_std( tf.stack(allUNSE) )) )\n",
    "          f.write('sttdev allVNSE = {}\\n'.format(tf.math.reduce_std( tf.stack(allVNSE) )) )\n",
    "          f.write('Pin2SigK = {}\\n'.format(tf.reduce_mean(tf.stack(Pin2SigK))) )\n",
    "          f.write('Pin2SigU = {}\\n'.format(tf.reduce_mean(tf.stack(Pin2SigU))) )\n",
    "          f.write('Pin2SigV = {}\\n'.format(tf.reduce_mean(tf.stack(Pin2SigV))) )\n",
    "          f.write('totalTimeTrainingPDLVM = {:.2f}m\\n'.format( totalTimeTrainingPDLVM ))\n",
    "     f.close()\n",
    "\n",
    "print('totalTimeTrainingPDLVM = {:.2f}m'.format( totalTimeTrainingPDLVM ) )\n",
    "\n",
    "\n",
    "\n",
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = True\n",
    "if saveData:\n",
    "\n",
    "    allSampleZs_np = tf.stack(allSampleZs)\n",
    "    allMeanZs_np   = tf.stack(allMeanZs)\n",
    "    allStdZs_np    = tf.stack(allStdZs)\n",
    "\n",
    "    allUSamples_np  = tf.stack(allUSamples)\n",
    "    allUMeans_np    = tf.stack(allUMeans)\n",
    "    allUStd_np      = tf.stack(allUStd)\n",
    "\n",
    "    prefix = 'randgrid{}_{}kIter'.format(nxSamples, int(num_iterations/1000))\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allRes_np.dat'   , tf.stack(tf.squeeze(allresAbsMean)) )\n",
    "\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allSampleZs_np.dat', allSampleZs_np)\n",
    "    np.savetxt(dirname + prefix + '_allMeanZs_np.dat'  , allMeanZs_np)\n",
    "    np.savetxt(dirname + prefix + '_allStdZs_np.dat'  , allMeanZs_np)\n",
    "\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allUSamples_np.dat' , allUSamples_np)\n",
    "    np.savetxt(dirname + prefix + '_allUMeans_np.dat'   , allUMeans_np)\n",
    "    np.savetxt(dirname + prefix + '_allUStd_np.dat'     , allUStd_np)\n",
    "    \n",
    "    np.savetxt(dirname + prefix + '_allZNSE_np.dat'   , tf.stack( allZNSE ) )\n",
    "    np.savetxt(dirname + prefix + '_allUNSE_np.dat'   , tf.stack( allUNSE ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in betaNet.NN.kipLayers:\n",
    "    print(layer.ell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "56d2bb5d7cc3676b4f24fb92af7a44f3b6495c3f95e7930a39b9feb733cbd04c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
