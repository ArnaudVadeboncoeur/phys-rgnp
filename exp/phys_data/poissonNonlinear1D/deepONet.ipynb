{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! If you want to force use of CPU (before import of tensorflow)\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "\n",
    "#! import as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "#! If you want to limit number of threads, the mechine has 32 virtual cores (16 physical ones)\n",
    "print('thread inter = ', tf.config.threading.get_inter_op_parallelism_threads() )\n",
    "print('thread intra = ', tf.config.threading.get_intra_op_parallelism_threads() )\n",
    " \n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "#! If you want to limit the GPU memory usage, the machine has ~24GB. For most applications a few GBs is enough.\n",
    "#! If you max out what you specify progressively up the threshhold.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.set_logical_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=6000)])\n",
    "\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "except:\n",
    "  # Invalid device or cannot modify logical devices once initialized.\n",
    "  pass\n",
    "\n",
    "#! Will print out what devices you are using\n",
    "tf.config.list_physical_devices()\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mlkernels.tensorflow import Matern32, EQ, Matern12, Matern52\n",
    "\n",
    "\n",
    "from LFM.NeuralNet import NeuralN\n",
    "from LFM.utilities import reparameterize, log_normal_pdf\n",
    "from LFM.GICNet    import FFLayer, KIPLayer, GICNet\n",
    "\n",
    "pi  = tf.constant(np.pi, tf.float32)\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "dirname='data-phys_data-DeepONet-PoissonNonLinear1D-trueGrid/'\n",
    "# dirname='data-DeepONet-PoissonNonLinear1D-diffGrid/'\n",
    "os.makedirs(dirname, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimU = 1\n",
    "dimZ = 5\n",
    "dimW = 1\n",
    "dimX = 1\n",
    "\n",
    "            # x      \n",
    "domain     = tf.constant( [-1., 1.] )\n",
    "\n",
    "# nxSamples  = 30\n",
    "# nxSamples  = 100\n",
    "nxSamples  = 300\n",
    "\n",
    "\n",
    "zDist = tfd.Uniform(low=tf.constant(-1., shape=[dimZ]), high=tf.constant(1., shape=[dimZ]))\n",
    "wDist = tfd.Uniform(low=tf.constant( 1., shape=[dimW]), high=tf.constant(2., shape=[dimW]))\n",
    "\n",
    "def raveled1DDomainGrid(resolution):\n",
    "    x  = tf.linspace(domain[0], domain[1], resolution[0])\n",
    "    return x[:, None]\n",
    "    \n",
    "\n",
    "xvect = raveled1DDomainGrid([nxSamples+2])\n",
    "gridX = xvect[1:-1,:]\n",
    "\n",
    "\n",
    "\n",
    "print('xGrid.shape = ', gridX.shape)\n",
    "print('xGrid = ', gridX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirnameData = '../../../fenics/poisson1DNonLinear/data-groundTruth_2--PoissonNonLinear1D_1000'\n",
    "\n",
    "dataU = tf.constant( np.loadtxt(dirnameData+'/' + 'allGroundTruthU.dat' ), dtype=tf.float32)\n",
    "dataZ = tf.constant( np.loadtxt(dirnameData+'/' + 'allGroundTruthZ.dat' ), dtype=tf.float32)\n",
    "dataW = tf.constant( np.loadtxt(dirnameData+'/' + 'allGroundTruthW.dat' ), dtype=tf.float32)\n",
    "dataX = tf.constant( np.loadtxt(dirnameData+'/' + 'FenicsMeshX.dat'     ), dtype=tf.float32)\n",
    "\n",
    "sigma_y         = tf.constant(0.05)\n",
    "nData_input     = 1000\n",
    "dataY           = ( dataU + tf.random.normal(stddev=sigma_y, shape=dataU.shape) )[:nData_input, 20:-20:3]\n",
    "\n",
    "batch_size_data = 50\n",
    "\n",
    "data_single_X     = tf.reshape(dataX, [-1,1])[20:-20:3, :]\n",
    "\n",
    "print('data_single_X.shape', data_single_X.shape)\n",
    "\n",
    "dimY            = dataY.shape[1]\n",
    "nData_y         = dataY.shape[0]\n",
    "print('dimY', dimY)\n",
    "\n",
    "dataset_data_YZWX_stack = tf.concat([dataY, dataZ[:nData_input,:], dataW[:nData_input,None]], 1)\n",
    "print('dataset_data_YZWX_stack.shape', dataset_data_YZWX_stack.shape)\n",
    "\n",
    "dataset_data_YZWX       = tf.data.Dataset.from_tensor_slices( dataset_data_YZWX_stack )\n",
    "dataset_data_YZWX_batched = dataset_data_YZWX.batch(batch_size=batch_size_data)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.scatter(data_single_X, dataY[i], s=5)\n",
    "    plt.plot(dataX, dataU[i])\n",
    "plt.tight_layout()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fw network\n",
    "\n",
    "nhfw             = 300\n",
    "dimBranchOutfw   = 300\n",
    "dimTruncOutfw    = 300\n",
    "dim_DeepO_outfw  = dimU\n",
    "\n",
    "fwNet  = NeuralN('fw', np.array([ 1, 1, dimZ]) )\n",
    "\n",
    "# numFreqfw = 100\n",
    "\n",
    "fwNet.branch   =  tf.keras.Sequential([ tf.keras.layers.Dense(  nhfw, input_dim=( dimZ + dimW ), activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( dimBranchOutfw ) ])\n",
    "\n",
    "fwNet.trunc   = tf.keras.Sequential([   tf.keras.layers.Dense(  nhfw,  input_dim=( dimX ), activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense(  nhfw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( dimTruncOutfw ) ])\n",
    "\n",
    "fwNet.final_bias = tf.Variable( tf.constant(0.), trainable=True)\n",
    "\n",
    "fwNet.lower_alpha = tf.constant(1e-5)\n",
    "fwNet.upper_w     = tf.constant(1.)\n",
    "\n",
    "def fwMap(self, z, w, x):\n",
    "\n",
    "        z       = tf.reshape(z, [-1, dimZ])\n",
    "        w       = tf.reshape(w, [-1, dimW])\n",
    "        x_trunc = tf.reshape(x, [-1, dimX])\n",
    "\n",
    "        x_trunc  = self.trunc( x_trunc ) # [nxSamples, dimTruncOut]\n",
    "        x_branch = self.branch( tf.concat([z, w], 1) ) # [1, dimBranchOut]\n",
    "\n",
    "        print('x_trunc.shape',  x_trunc.shape)\n",
    "        print('x_branch.shape', x_branch.shape)\n",
    "        \n",
    "        mapped = tf.einsum('bi,ni->bn', x_trunc, x_branch ) + self.final_bias\n",
    "\n",
    "        print('mapped.shape', mapped.shape)\n",
    "\n",
    "        tf.debugging.assert_all_finite(mapped, 'mapped not finite')\n",
    "        \n",
    "        return mapped\n",
    "\n",
    "from types import MethodType\n",
    "fwNet.call = MethodType(fwMap, fwNet)\n",
    "outfw = fwNet(tf.constant(1., shape=[dimZ]), tf.constant(1., shape=[dimW]), gridX)\n",
    "\n",
    "print('outfw.shape', outfw.shape)\n",
    "\n",
    "fwNet.compile()\n",
    "fwNet.NN.summary()\n",
    "fwNet.summary()\n",
    "\n",
    "# fwNet.save_weights(dirname + '/fwModel/fwModel')\n",
    "# fwNet.load_weights(dirname + '/fwModel/fwModel')\n",
    "\n",
    "# del fwNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def D(x):\n",
    "    return tf.cos(0.5 * pi * x)\n",
    "\n",
    "epsilon_r = tf.Variable(1e-2, trainable=False)\n",
    "\n",
    "Nz = tf.range(0, dimZ, dtype=tf.float32)\n",
    "\n",
    "def cheby(n, x):\n",
    "    return tf.cos( n * tf.math.acos(x) )\n",
    "\n",
    "def bound( xd ): return cheby(Nz, xd)\n",
    "\n",
    "def kappa(z, u, x):\n",
    "    Txn  =  tf.vectorized_map(bound, x)\n",
    "    return tf.math.log( 1. + tf.exp( tf.reshape(u, [-1,1]) * tf.reshape(Txn @ tf.reshape(z, [-1,1]), [-1,1]) ) ) + 0.1\n",
    "\n",
    "\n",
    "x = tf.linspace(-1., 1., 100)[:, None]\n",
    "z = zDist.sample()\n",
    "plt.plot(tf.linspace(-1., 1., 100), kappa(z, x**0., x) )\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "def forcing(w, x):\n",
    "    return tf.reshape(w, [-1]) + x * 0.\n",
    "\n",
    "\n",
    "def residualFunction( zw ):\n",
    "    \n",
    "    # x, t = xrand[:,:1], xrand[:, 1:]\n",
    "    print('zw', zw)\n",
    "    z, w = zw[ :dimZ ], zw[ dimZ:dimZ+dimW ]\n",
    "    z    = tf.reshape(z, [-1,1])\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t1:\n",
    "        t1.watch( gridX )\n",
    "\n",
    "        with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t2:\n",
    "            t2.watch( gridX )\n",
    "\n",
    "            outfw = fwNet( z, w, gridX )\n",
    "\n",
    "            u = D( gridX ) * outfw\n",
    "\n",
    "            k = kappa(z, u, gridX)\n",
    "            \n",
    "        u_x = tf.reshape(t2.gradient(u, gridX), [-1,1])\n",
    "        k_x = tf.reshape(t2.gradient(k, gridX), [-1,1])\n",
    "    \n",
    "    u_xx = tf.reshape(t1.gradient(u_x, gridX), [-1,1])\n",
    "\n",
    "    f = forcing(w, gridX)\n",
    "    \n",
    "    res = tf.reduce_mean( (k_x * u_x + k * u_xx + f)**2. )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "z     =  zDist.sample(  ) \n",
    "w     =  wDist.sample(  ) \n",
    "print('z', z)\n",
    "print('w', w)\n",
    "residualFunction( tf.concat([z,w],0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_data( YZW ):\n",
    "\n",
    "    y, z, w = YZW[:dimY], YZW[dimY:dimY+dimZ], YZW[dimY+dimZ:]\n",
    "    y       = tf.reshape(y, [-1,1])\n",
    "\n",
    "    print('y.shape', y.shape)\n",
    "    print('z.shape', z.shape)\n",
    "    print('w.shape', w.shape)\n",
    "\n",
    "\n",
    "    outfw = fwNet( z, w, data_single_X )\n",
    "\n",
    "    Dvect = D( data_single_X )\n",
    "\n",
    "    outfw_trans = Dvect * outfw\n",
    "\n",
    "    return  tf.reduce_mean(  (tf.reshape(y, shape=outfw_trans.shape) - outfw_trans)**2. )\n",
    "\n",
    "res_data(dataset_data_YZWX_stack[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainableVarsfw   = [ fwNet.trainable_variables ]\n",
    "\n",
    "@tf.function\n",
    "def train_step_fw(optimizer, batch_phys, batch_data):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "    \n",
    "        losses_phys = tf.vectorized_map( residualFunction, batch_phys )\n",
    "        loss_phys = tf.reduce_mean(losses_phys)\n",
    "        \n",
    "        losses_data = tf.vectorized_map( res_data, batch_data )\n",
    "        loss_data   = nData_y / batch_size_data * tf.reduce_sum(losses_data)\n",
    "        \n",
    "        print('loss_data = ', loss_data)\n",
    "        \n",
    "        loss = loss_phys + loss_data\n",
    "\n",
    "    for vars in trainableVarsfw:\n",
    "        gradients = tape.gradient(loss, vars)\n",
    "        optimizer.apply_gradients(zip(gradients, vars))\n",
    "\n",
    "    return loss, loss_data, loss_phys\n",
    "\n",
    "batchSize = 50\n",
    "nData     = 1_000\n",
    "datasetZ  = zDist.sample( nData )\n",
    "datasetW  = wDist.sample( nData )\n",
    "datasetZW = tf.concat([datasetZ, datasetW], 1)\n",
    "dataset_phys        = tf.data.Dataset.from_tensor_slices( datasetZW )\n",
    "datasetBatched_phys = dataset_phys.batch(batch_size=batchSize)\n",
    "\n",
    "num_iterations   = tf.constant(1_000_000)\n",
    "epochs           = int(num_iterations / nData)\n",
    "\n",
    "intervalEPOCHsave = 100\n",
    "\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    1e-3, int(num_iterations/10), 0.5, staircase=True, name=None\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam( lr )\n",
    "\n",
    "lossfw_all     = []\n",
    "startTrainingfw = time.time()\n",
    "\n",
    "iteration = tf.constant(0)\n",
    "for epoch in tf.range( epochs ):\n",
    "    start_time = time.time()\n",
    "    for batch_phys, batch_data in zip(datasetBatched_phys, dataset_data_YZWX_batched):\n",
    "        iteration += batchSize\n",
    "        loss, loss_data, loss_phys = train_step_fw(optimizer, batch_phys, batch_data )\n",
    "    if iteration >= num_iterations: \n",
    "        print('DONE BREAK')\n",
    "        break\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if epoch % intervalEPOCHsave == 0:\n",
    "        lossfw_all.append( -loss ) \n",
    "        print('Iteration: {}, Epoch: {}, ELBO: {:.2f}, loss_phys: {:.2f}, loss_data: {:.2f}, time elapse for current epoch: {:.2f}'\n",
    "           .format(iteration, epoch, -loss, -loss_phys, -loss_data, end_time - start_time))\n",
    "\n",
    "\n",
    "timeTrainfw = (time.time() - startTrainingfw)/60\n",
    "print('totalTimeTraining = {:.2f}m'.format(timeTrainfw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfw_all = tf.stack(lossfw_all)\n",
    "LOSS_fw    = np.array(lossfw_all)\n",
    "plt.plot(range(LOSS_fw.shape[0]), LOSS_fw )\n",
    "plt.show()\n",
    "print(np.isnan(LOSS_fw).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bw network\n",
    "\n",
    "dvbw             = 16\n",
    "nh_bw            = 300\n",
    "dimBranchOutbw   = 300\n",
    "dimTruncOutbw    = 300\n",
    "dim_DeepO_outbw  = dimZ\n",
    "\n",
    "bwNet  = NeuralN('bw', np.array([ 1, 1, dimZ]) )\n",
    "\n",
    "\n",
    "bwNet.trunc   =  tf.keras.Sequential([  tf.keras.layers.Dense( nh_bw, input_dim=( dimW ), activation='swish'),\n",
    "                                        tf.keras.layers.Dense( nh_bw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( nh_bw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( nh_bw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( nh_bw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( nh_bw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( nh_bw, activation='swish'),\n",
    "                                        tf.keras.layers.Dense( dimTruncOutbw ) ])\n",
    "\n",
    "bwNet.branch   = tf.keras.Sequential([  tf.keras.layers.Conv1D( dvbw, (3), strides=(1), activation='swish' ),\n",
    "                                        tf.keras.layers.Conv1D( dvbw, (3), strides=(1), activation='swish' ),\n",
    "                                        tf.keras.layers.Conv1D( dvbw, (3), strides=(1), activation='swish' ),\n",
    "                                        tf.keras.layers.Conv1D( dvbw, (3), strides=(1), activation='swish' ),\n",
    "                                        tf.keras.layers.Conv1D( dvbw, (3), strides=(1), activation='swish' ),\n",
    "                                        tf.keras.layers.Conv1D( dvbw, (3), strides=(1), activation='swish' ),\n",
    "                                        tf.keras.layers.Flatten(  ),\n",
    "                                        tf.keras.layers.Dense(500),\n",
    "                                        tf.keras.layers.Dense(dimBranchOutbw) ])\n",
    "\n",
    "bwNet.final_bias = tf.Variable( tf.constant(0., shape=[dimZ]), trainable=True)\n",
    "\n",
    "bwNet.lower_alpha = tf.constant(1e-5)\n",
    "bwNet.upper_w     = tf.constant(1.)\n",
    "\n",
    "def bwMap(self, u, w, x):\n",
    "\n",
    "        branch_in = tf.concat([x, u], 1)\n",
    "        trunc_in  = tf.reshape(w, [-1,dimW])\n",
    "\n",
    "        x_trunc  = self.trunc( trunc_in ) # [nxSamples, dimTruncOut]\n",
    "        x_branch = self.branch( branch_in[None, ...] ) # [1, dimBranchOut]\n",
    "\n",
    "        print('x_trunc.shape',  x_trunc.shape)\n",
    "        print('x_branch.shape', x_branch.shape)\n",
    "\n",
    "        x_trunc  = tf.reshape(x_trunc,  [-1, dimZ])\n",
    "        x_branch = tf.reshape(x_branch, [-1, dimZ])\n",
    "\n",
    "        print('x_trunc.shape',  x_trunc.shape)\n",
    "        print('x_branch.shape', x_branch.shape)\n",
    "        \n",
    "        mapped = tf.einsum('ij,ij->j', x_trunc, x_branch ) + self.final_bias\n",
    "        mapped = tf.reshape(mapped, [1,-1])\n",
    "        \n",
    "        print('mapped.shape', mapped.shape)\n",
    "\n",
    "        tf.debugging.assert_all_finite(mapped, 'mapped not finite')\n",
    "        \n",
    "        return mapped\n",
    "\n",
    "from types import MethodType\n",
    "bwNet.call = MethodType(bwMap, bwNet)\n",
    "bwNet(tf.constant(1., shape=[nxSamples, dimU]), tf.constant(1., shape=[dimW]), gridX)\n",
    "bwNet.compile()\n",
    "bwNet.summary()\n",
    "\n",
    "# bwNet.save_weights(dirname + '/bwModel/bwModel')\n",
    "# bwNet.load_weights(dirname + '/bwModel/bwModel')\n",
    "\n",
    "# del bwNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(zw):\n",
    "    z, w = zw[ :dimZ ], zw[ dimZ:dimZ+dimW ]\n",
    "    return tf.squeeze( fwNet( z, w, gridX ))    \n",
    "\n",
    "datasetUfw = tf.vectorized_map(bind, datasetZW )\n",
    "print(datasetUfw.shape)\n",
    "\n",
    "datasetUWZ        = tf.concat([datasetUfw, datasetW, datasetZ], 1)\n",
    "datasetUWZBatched =  tf.data.Dataset.from_tensor_slices( datasetUWZ ).batch(batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bwLoss( u_untranswz ):\n",
    "    u_untrans, w, z = u_untranswz[ :nxSamples ], u_untranswz[ nxSamples:nxSamples+dimW ], u_untranswz[nxSamples+dimW:]\n",
    "    print('u_untrans.shape', u_untrans)\n",
    "    print('w.shape', w)\n",
    "    print('z.shape', z)\n",
    "    predbw    = bwNet( tf.reshape(u_untrans, [-1,1]), w , gridX)\n",
    "    loss      = tf.reduce_mean(  (tf.reshape( predbw, [-1] ) - tf.reshape( z, [-1] ))**2. )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainableVarsbw   = [ bwNet.trainable_variables ]\n",
    "\n",
    "@tf.function\n",
    "def train_step_bw(optimizer, batch):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        losses = tf.vectorized_map( bwLoss, batch )\n",
    "        loss   = tf.reduce_mean( losses )\n",
    "    for vars in trainableVarsbw:\n",
    "        gradients = tape.gradient(loss, vars)\n",
    "        optimizer.apply_gradients(zip(gradients, vars))\n",
    "\n",
    "    return loss\n",
    "\n",
    "num_iterations_bw = num_iterations\n",
    "\n",
    "intervalEPOCHsave = 100\n",
    "\n",
    "lr_bw = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    1e-3, int(num_iterations_bw/10), 0.5, staircase=True, name=None\n",
    ")\n",
    "optimizer_bw = tf.keras.optimizers.Adam( lr_bw )\n",
    "\n",
    "lossAll_bw     = []\n",
    "startTrainingDeepO_bw = time.time()\n",
    "\n",
    "iteration_bw = tf.constant(0)\n",
    "startTrainingbw = time.time()\n",
    "for epoch in tf.range( epochs ):\n",
    "    start_time = time.time()\n",
    "    for batch in datasetUWZBatched:\n",
    "        iteration_bw += batchSize\n",
    "        loss = train_step_bw(optimizer_bw, batch)\n",
    "    if iteration_bw >= num_iterations_bw: \n",
    "        print('DONE BREAK')\n",
    "        break\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if epoch % intervalEPOCHsave == 0:\n",
    "        lossAll_bw.append( -loss ) \n",
    "        print('Iter: {}, Epoch: {}, Test set loss: {}, time elapse for current epoch: {:.2f}'\n",
    "           .format(iteration_bw, epoch, -loss, end_time - start_time))\n",
    "\n",
    "\n",
    "timeTrainbw = (time.time() - startTrainingbw)/60\n",
    "print('totalTimeTraining = {:.2f}m'.format(timeTrainbw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossAll_bw = tf.stack(lossAll_bw)\n",
    "LOSS_bw    = np.array(lossAll_bw)\n",
    "plt.plot(range(LOSS_bw.shape[0]), LOSS_bw )\n",
    "plt.show()\n",
    "print(np.isnan(LOSS_bw).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('totTimetraining = ', timeTrainfw + timeTrainbw)\n",
    "\n",
    "dirnameGT = '../../../fenics/poisson1DNonLinear/data-groundTruth--PoissonNonLinear1D_1000'\n",
    "\n",
    "\n",
    "groundTruthU = tf.constant( np.loadtxt(dirnameGT+'/' + 'allGroundTruthU.dat' ), dtype=tf.float32)\n",
    "allTrueZs    = tf.constant( np.loadtxt(dirnameGT+'/' + 'allGroundTruthZ.dat' ), dtype=tf.float32)\n",
    "allTrueWs    = tf.constant( np.loadtxt(dirnameGT+'/' + 'allGroundTruthW.dat' ), dtype=tf.float32)\n",
    "groundTruthX = tf.constant( np.loadtxt(dirnameGT+'/' + 'FenicsMeshX.dat'     ), dtype=tf.float32)\n",
    "\n",
    "saveDataList = True\n",
    "\n",
    "if saveDataList:\n",
    "\n",
    "     allresAbsMean = []\n",
    "\n",
    "     allPredZs     = []\n",
    "\n",
    "     allUPreds     = []\n",
    "\n",
    "     allZNSE       = []\n",
    "     allUNSE       = []\n",
    "\n",
    "kip = KIPLayer(EQ(), 1./nxSamples)\n",
    "\n",
    "Xtest   = groundTruthX[:, None]\n",
    "\n",
    "PLOT=False\n",
    "\n",
    "tf_residualFunction = tf.function( residualFunction )\n",
    "tf_fwNet = tf.function( fwNet )\n",
    "tf_bwNet  = tf.function( bwNet )\n",
    "\n",
    "# Xtest   = gridX\n",
    "for i in range(groundTruthU.shape[0]):\n",
    "\n",
    "     zTrue = allTrueZs[i]\n",
    "     wTrue = allTrueWs[i][None]\n",
    "     uTrue = groundTruthU[i]\n",
    "     \n",
    "     residual = tf_residualFunction( tf.concat([zTrue, wTrue], 0) )\n",
    "     u_untrans = tf_fwNet( zTrue, wTrue, Xtest )\n",
    "\n",
    "     u = tf.squeeze( D( Xtest ) * u_untrans )\n",
    "\n",
    "     if PLOT:\n",
    "          c = next(plt.gca()._get_lines.prop_cycler)['color']\n",
    "\n",
    "          plt.plot(Xtest, u,  '--', linewidth=1,  c=c)\n",
    "          plt.plot(groundTruthX[:, None], uTrue,   linewidth=1,    c=c)\n",
    "     \n",
    "     interpedU = kip.call(Xtest, tf.reshape(uTrue/D(tf.squeeze(Xtest)), [-1,1]), gridX)\n",
    "     z = tf.reshape( tf_bwNet( tf.reshape(interpedU, [-1,1]), wTrue,  gridX ), [-1] )\n",
    "\n",
    "     print('z = ', z)\n",
    "     print('w = ', wTrue)\n",
    "     print('true z = '   , zTrue)\n",
    "     print('\\n\\n')\n",
    "\n",
    "\n",
    "     if saveDataList:\n",
    "\n",
    "          allresAbsMean.append( tf.squeeze(residual) ) \n",
    "\n",
    "          allUPreds.append( tf.squeeze(u) )\n",
    "\n",
    "          allPredZs.append( tf.squeeze(z) )\n",
    "          \n",
    "          allZNSE.append( ( tf.linalg.norm((tf.squeeze(z) - tf.squeeze(zTrue)) ) / tf.linalg.norm(tf.squeeze(zTrue)) )**2. )\n",
    "\n",
    "          allUNSE.append( ( tf.linalg.norm((tf.squeeze(u) - tf.squeeze(uTrue)) ) / tf.linalg.norm(tf.squeeze(uTrue)) )**2. )\n",
    "\n",
    "if PLOT:\n",
    "     plt.xlabel(r'$x$')\n",
    "     plt.ylabel(r'$u(x)$')\n",
    "     plt.tight_layout()\n",
    "     plt.show()\n",
    "\n",
    "\n",
    "print('mean allZNSE = ', tf.reduce_mean( tf.stack(allZNSE) ) )\n",
    "print('mean allUNSE = ', tf.reduce_mean( tf.stack(allUNSE) ) )\n",
    "\n",
    "print('sttdev allZNSE = ', tf.math.reduce_std( tf.stack(allZNSE) ) )\n",
    "print('sttdev allUNSE = ', tf.math.reduce_std( tf.stack(allUNSE) ) )\n",
    "print('totTimetraining = ', timeTrainfw + timeTrainbw)\n",
    "\n",
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fwNet.save_weights(dirname + '/fwModel/fwModel')\n",
    "    # alphaNet.load_weights(dirname + '/fwModel/fwModel')\n",
    "\n",
    "    # del alphaNet\n",
    "if True:\n",
    "    bwNet.save_weights(dirname + '/bwModel/bwModel')\n",
    "    # betaNet.load_weights(dirname + '/bwModel/bwModel')\n",
    "\n",
    "    # del betaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = True\n",
    "if saveData:\n",
    "\n",
    "    prefix = 'randgrid{}'.format(nxSamples)\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allRes_np.dat'   , tf.stack(tf.squeeze(allresAbsMean)) )\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allPredZs.dat', tf.stack(allPredZs))\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allUPreds.dat' , tf.stack(allUPreds))\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allZNSE.dat'   , tf.stack( allZNSE ) )\n",
    "    np.savetxt(dirname + prefix + '_allUNSE.dat'   , tf.stack( allUNSE ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('lfm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "56d2bb5d7cc3676b4f24fb92af7a44f3b6495c3f95e7930a39b9feb733cbd04c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
