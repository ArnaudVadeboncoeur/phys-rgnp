{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! If you want to force use of CPU (before import of tensorflow)\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "\n",
    "#! import as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "#! If you want to limit number of threads, the mechine has 32 virtual cores (16 physical ones)\n",
    "print('thread inter = ', tf.config.threading.get_inter_op_parallelism_threads() )\n",
    "print('thread intra = ', tf.config.threading.get_intra_op_parallelism_threads() )\n",
    " \n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "#! If you want to limit the GPU memory usage, the machine has ~24GB. For most applications a few GBs is enough.\n",
    "#! If you max out what you specify progressively up the threshhold.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.set_logical_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=6000)])\n",
    "\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "except:\n",
    "  # Invalid device or cannot modify logical devices once initialized.\n",
    "  pass\n",
    "\n",
    "#! Will print out what devices you are using\n",
    "tf.config.list_physical_devices()\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mlkernels.tensorflow import Matern32, EQ, Matern12, Matern52\n",
    "\n",
    "\n",
    "from LFM.NeuralNet import NeuralN\n",
    "from LFM.utilities import reparameterize, log_normal_pdf, expHighLowFunc_01\n",
    "from LFM.GICNet    import FFLayer, KIPLayer, GICNet\n",
    "\n",
    "pi  = tf.constant(np.pi, tf.float32)\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "dirname='data-phys_data-lfmDiag-PoissonNonLinear1D/'\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dimU = 1\n",
    "dimZ = 5\n",
    "dimW = 1\n",
    "dimX = 1\n",
    "\n",
    "            # x      \n",
    "domain     = tf.constant( [-1., 1.] )\n",
    "\n",
    "nxSamples  = 30\n",
    "\n",
    "zDist = tfd.Uniform(low=tf.constant(-1., shape=[dimZ]), high=tf.constant(1., shape=[dimZ]))\n",
    "wDist = tfd.Uniform(low=tf.constant( 1., shape=[dimW]), high=tf.constant(2., shape=[dimW]))\n",
    "e_bound = 1e-5\n",
    "xDist = tfd.Uniform(low=domain[0]+e_bound, high=domain[1]-e_bound) \n",
    "\n",
    "b     = zDist.sample()\n",
    "xvect = xDist.sample(sample_shape=(nxSamples, dimX))\n",
    "\n",
    "def raveled1DDomainGrid(resolution):\n",
    "    x  = tf.linspace(domain[0], domain[1], resolution[0])\n",
    "    return x[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirnameGT = 'data-trueEq--PoissonNonLinear1D_1000'\n",
    "# dirnameGT = '../../fenics/exp/poisson1DNonLinear-Comparisons/data-groundTruth--PoissonNonLinear1D_1000'\n",
    "dirnameData = '../../../fenics/poisson1DNonLinear/data-groundTruth_2--PoissonNonLinear1D_1000'\n",
    "\n",
    "\n",
    "dataU = tf.constant( np.loadtxt(dirnameData+'/' + 'allGroundTruthU.dat' ), dtype=tf.float32)\n",
    "dataZ = tf.constant( np.loadtxt(dirnameData+'/' + 'allGroundTruthZ.dat' ), dtype=tf.float32)\n",
    "dataW = tf.constant( np.loadtxt(dirnameData+'/' + 'allGroundTruthW.dat' ), dtype=tf.float32)\n",
    "dataX = tf.constant( np.loadtxt(dirnameData+'/' + 'FenicsMeshX.dat'     ), dtype=tf.float32)\n",
    "\n",
    "sigma_y         = tf.constant(0.05)\n",
    "nData_input     = 1000\n",
    "# dataY           = ( dataU + tf.random.normal(stddev=sigma_y, shape=dataU.shape) )[:, 1:-1]\n",
    "dataY           = ( dataU + tf.random.normal(stddev=sigma_y, shape=dataU.shape) )[:nData_input, 20:-20:3]\n",
    "\n",
    "batch_size_data = 50\n",
    "\n",
    "# data_single_X     = tf.reshape(dataX, [-1,1])[1:-1,:]\n",
    "data_single_X     = tf.reshape(dataX, [-1,1])[20:-20:3, :]\n",
    "\n",
    "print('data_single_X.shape', data_single_X.shape)\n",
    "\n",
    "dimY            = dataY.shape[1]\n",
    "nData_y         = dataY.shape[0]\n",
    "print('dimY', dimY)\n",
    "\n",
    "dataset_data_YZWX_stack = tf.concat([dataY, dataZ[:nData_input,:], dataW[:nData_input,None]], 1)\n",
    "print('dataset_data_YZWX_stack.shape', dataset_data_YZWX_stack.shape)\n",
    "\n",
    "dataset_data_YZWX       = tf.data.Dataset.from_tensor_slices( dataset_data_YZWX_stack )\n",
    "dataset_data_YZWX_batched = dataset_data_YZWX.batch(batch_size=batch_size_data)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.scatter(data_single_X, dataY[i], s=5)\n",
    "    plt.plot(dataX, dataU[i])\n",
    "plt.tight_layout()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('5Data_ySamples_trueLine_scatterData.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha network\n",
    "\n",
    "dim_DeepO_out  = dimU * 2\n",
    "\n",
    "alphaNet  = NeuralN('alpha', np.array([ 1, 1, dimZ]) )\n",
    "\n",
    "numFreq = 100\n",
    "\n",
    "nhAlpha = 300\n",
    "\n",
    "alphaNet.NN = tf.keras.Sequential([ tf.keras.layers.Dense( nhAlpha, input_dim=( numFreq * 2 + dimZ + dimW ), activation='swish'),\n",
    "                                    tf.keras.layers.Dense( nhAlpha, activation='swish'),\n",
    "                                    tf.keras.layers.Dense( nhAlpha, activation='swish'),\n",
    "                                    tf.keras.layers.Dense( nhAlpha, activation='swish'),\n",
    "                                    tf.keras.layers.Dense( nhAlpha, activation='swish'),\n",
    "                                    tf.keras.layers.Dense( nhAlpha, activation='swish'),\n",
    "                                    tf.keras.layers.Dense( nhAlpha, activation='swish'),\n",
    "                                    tf.keras.layers.Dense( dim_DeepO_out ) ])\n",
    "\n",
    "freqsX = tf.linspace(0., 1., numFreq)\n",
    "freqs  = expHighLowFunc_01(2., 0.001, freqsX)\n",
    "tf.print('all freqs FFN \\n', freqs, summarize=-1) \n",
    "fLayer = [ FFLayer( freqs ) ]\n",
    "\n",
    "alphaNet.lower_alpha = tf.constant(1e-5)\n",
    "alphaNet.upper_w     = tf.constant(1.)\n",
    "\n",
    "def alphaMap(self, z, w, x):\n",
    "\n",
    "        z_tile  = tf.tile(tf.reshape(z, [1,-1]), [x.shape[0], 1])\n",
    "        w_tile  = tf.tile(tf.reshape(w, [1,-1]), [x.shape[0], 1])\n",
    "        xFreq   = fLayer[0]( x )\n",
    "        x_input = tf.concat([xFreq, z_tile, w_tile], 1)\n",
    "        mapped  = self.NN( x_input )\n",
    "        \n",
    "        mean, logvar = mapped[:,:dimU], mapped[:,dimU:2*dimU]\n",
    "        \n",
    "        tf.debugging.assert_all_finite(mean, 'mean not finite')\n",
    "        tf.debugging.assert_all_finite(logvar, 'logvar not finite')\n",
    "\n",
    "        logvar = self.dBoundwalpha_log( logvar )\n",
    "\n",
    "        print('alpha mean.shape = ',   mean.shape)\n",
    "        print('alpha logvar.shape = ', logvar.shape)\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "from types import MethodType\n",
    "alphaNet.call = MethodType(alphaMap, alphaNet)\n",
    "alphaNet(tf.constant(1., shape=[dimZ]), tf.constant(1., shape=[dimW]), tf.constant(1., shape=[nxSamples,dimX]))\n",
    "alphaNet.compile()\n",
    "alphaNet.NN.summary()\n",
    "alphaNet.summary()\n",
    "\n",
    "# alphaNet.save_weights(dirname + '/alphaModel/alphaModel-lfmDiag')\n",
    "# alphaNet.load_weights(dirname + '/alphaModel/alphaModel-lfmDiag')\n",
    "\n",
    "# del alphaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define beta network\n",
    "\n",
    "dim_trunc_in_beta   = dimW\n",
    "dim_trunc_out_beta  = 100 \n",
    "\n",
    "dim_branch_in_y_beta = 1\n",
    "dim_branch_P_beta    = 20\n",
    "dv_beta              = 20 # number of channels -- filters\n",
    "dim_branch_out_beta  = 100 #dimZ #20 #works\n",
    "\n",
    "\n",
    "dim_DeepO_in_beta   = dim_trunc_out_beta + dim_branch_out_beta\n",
    "dim_DeepO_out_beta  = dimZ * 2\n",
    "\n",
    "kip_res       = tf.constant([20])\n",
    "grid_kip_beta = raveled1DDomainGrid( kip_res )\n",
    "\n",
    "gicNetBeta = GICNet( grid_kip_beta, kip_res, dim_branch_in_y_beta, dim_branch_P_beta, dimX  )\n",
    "\n",
    "gicNetBeta.P_kLayers   = [ tf.keras.layers.Dense( dim_branch_P_beta, input_dim=( dim_branch_in_y_beta ), activation ='swish'),\n",
    "                           tf.keras.layers.Dense( dim_branch_P_beta, activation ='swish'),\n",
    "                           tf.keras.layers.Dense( dim_branch_P_beta, activation ='swish'),\n",
    "                           tf.keras.layers.Dense( dim_branch_P_beta, activation ='swish') ]\n",
    "                            \n",
    "gicNetBeta.kipLayers   = []\n",
    "ellXs = tf.linspace(0., 1., dim_branch_P_beta)\n",
    "low  = 1e-3\n",
    "high = 1.\n",
    "ellVals     = expHighLowFunc_01(low, high, ellXs)\n",
    "for i in range(dim_branch_P_beta):\n",
    "\n",
    "        gicNetBeta.kipLayers.append(KIPLayer(EQ(), ellVals[i]))\n",
    "        print('kip layer {} ell = '.format(i), gicNetBeta.kipLayers[i].ell)\n",
    "\n",
    "\n",
    "gicNetBeta.FFMlayer    = [ ]\n",
    "\n",
    "gicNetBeta.preTrunclayers  = [ \n",
    "                         tf.keras.layers.Dense( dim_trunc_out_beta, input_dim=( dim_trunc_in_beta ), activation ='swish'), \n",
    "                         tf.keras.layers.Dense( dim_trunc_out_beta,                                  activation ='swish'), \n",
    "                         tf.keras.layers.Dense( dim_trunc_out_beta,                                  activation ='swish')\n",
    "                         ]\n",
    "\n",
    "gicNetBeta.convLayers = [ \n",
    "                        tf.keras.layers.Conv1D( dv_beta, (3), strides=(1), activation='swish' ),\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (3), strides=(1), activation='swish' ),\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (3), strides=(1), activation='swish' ),\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (3), strides=(1), activation='swish' ),\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (3), strides=(1), activation='swish' ),\n",
    "                        tf.keras.layers.Conv1D( dv_beta, (3), strides=(1), activation='swish' ),\n",
    "                        tf.keras.layers.Flatten(  ),\n",
    "                        tf.keras.layers.Dense(500),\n",
    "                        tf.keras.layers.Dense(dim_branch_out_beta),\n",
    "                        ]\n",
    "                        \n",
    "# nn  = tf.keras.Sequential( gicNetBeta.convLayers  )\n",
    "# val = tf.random.uniform(shape=[1,kip_res[0], dim_branch_P_beta + dimX])\n",
    "# nn(val)\n",
    "# nn.summary()\n",
    "# nn = None\n",
    "\n",
    "nhBeta       = 300\n",
    "deep0_beta   = tf.keras.layers.Dense( nhBeta, input_dim=( dim_DeepO_in_beta ), activation=tf.keras.activations.swish)\n",
    "deep1_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep2_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep3_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep2_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deep3_beta   = tf.keras.layers.Dense( nhBeta, activation=tf.keras.activations.swish)\n",
    "deepOut_beta = tf.keras.layers.Dense( dim_DeepO_out_beta )\n",
    "gicNetBeta.DeepOlayers = [ deep0_beta, deep1_beta, deep2_beta, deep3_beta, deepOut_beta ]\n",
    "\n",
    "\n",
    "betaNet    = NeuralN('beta', np.array([ 1, 1, dimZ]) )\n",
    "betaNet.NN = gicNetBeta\n",
    "\n",
    "betaNet.lower_alpha  = tf.constant(1e-5) \n",
    "betaNet.upper_w      = tf.constant(1.)\n",
    "\n",
    "def betaMap(self, u, w, x):\n",
    "\n",
    "        trunc_in  = tf.reshape(w, [-1,dimW])\n",
    "        branch_in = tf.concat([x, u], 1)\n",
    "\n",
    "        mapped = self.NN( branch_in, trunc_in )\n",
    "        print('mapped.shape = ', mapped.shape)\n",
    "\n",
    "        mean, logvar = mapped[:,:dimZ], mapped[:,dimZ:2*dimZ]\n",
    "\n",
    "        tf.debugging.assert_all_finite(mean, 'mean not finite')\n",
    "        tf.debugging.assert_all_finite(logvar, 'logvar not finite')\n",
    "\n",
    "        logvar = self.dBoundwalpha_log( logvar )\n",
    "        print('beta mean.shape = ', mean.shape)\n",
    "        print('beta logvar.shape = ', logvar.shape)\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "from types import MethodType\n",
    "betaNet.call = MethodType(betaMap, betaNet)\n",
    "\n",
    "mean_alpha, logvar_alpha = alphaNet(tf.constant(1., shape=[dimZ]), tf.constant(1., shape=[dimW]), xvect)\n",
    "print(mean_alpha.shape)\n",
    "\n",
    "mean_beta, logvar_beta = betaNet( mean_alpha, tf.constant(1., shape=[dimW]), xvect )\n",
    "betaNet.compile( )\n",
    "betaNet.summary( )\n",
    "betaNet.NN.summary( )\n",
    "\n",
    "# betaNet.save_weights(dirname + '/betaModel/betaModel-lfmDiag')\n",
    "# betaNet.load_weights(dirname + '/betaModel/betaModel-lfmDiag')\n",
    "\n",
    "# del betaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def D(x):\n",
    "    return tf.cos(0.5 * pi * x)\n",
    "\n",
    "epsilon_r = tf.Variable(1e-2, trainable=False)\n",
    "\n",
    "Nz = tf.range(0, dimZ, dtype=tf.float32)\n",
    "\n",
    "def cheby(n, x):\n",
    "    return tf.cos( n * tf.math.acos(x) )\n",
    "\n",
    "def bound( xd ): return cheby(Nz, xd)\n",
    "\n",
    "def kappa(z, u, x):\n",
    "    Txn  =  tf.vectorized_map(bound, x)\n",
    "    return tf.math.log( 1. + tf.exp( tf.reshape(u, [-1,1]) * tf.reshape(Txn @ tf.reshape(z, [-1,1]), [-1,1]) ) ) + 0.1\n",
    "\n",
    "x = tf.linspace(-1., 1., 100)[:, None]\n",
    "z = zDist.sample()\n",
    "plt.plot(tf.linspace(-1., 1., 100), kappa(z, x**0., x) )\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "def forcing(w, x):\n",
    "    return w + x * 0.\n",
    "\n",
    "# @tf.function\n",
    "def residualFunction( z, w, x ):\n",
    "    \n",
    "    # x, t = xrand[:,:1], xrand[:, 1:]\n",
    "    x = tf.reshape(x, [-1,1])\n",
    "    z = tf.reshape(z, [-1,1])\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t1:\n",
    "        t1.watch( x )\n",
    "\n",
    "        with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t2:\n",
    "            t2.watch( x )\n",
    "\n",
    "            mean_alpha, logvar_alpha = alphaNet( z, w, x )\n",
    "            u_untrans = tf.reshape(reparameterize(mean_alpha, logvar_alpha), [-1,1])\n",
    "\n",
    "            u = D(x) * u_untrans\n",
    "\n",
    "            k = kappa(z, u, x)\n",
    "            \n",
    "        u_x = tf.reshape(t2.gradient(u, x), [-1,1])\n",
    "        k_x = tf.reshape(t2.gradient(k, x), [-1,1])\n",
    "    \n",
    "    u_xx = tf.reshape(t1.gradient(u_x, x), [-1,1])\n",
    "\n",
    "    f = forcing(w, x)\n",
    "    \n",
    "    # true residual\n",
    "    res = k_x * u_x + k * u_xx + f\n",
    "    \n",
    "    print(\"residualBody.shape\", res.shape)\n",
    "    print('u.shape', u.shape)\n",
    "    print('mean_alpha.shape', mean_alpha.shape)\n",
    "    print('logvar_alpha.shape', logvar_alpha.shape )\n",
    "        \n",
    "    logP_alpha_u_Z = log_normal_pdf(u_untrans, mean_alpha, logvar_alpha, raxis=0)\n",
    "\n",
    "    return u_untrans, u, res, logP_alpha_u_Z\n",
    "\n",
    "\n",
    "def ELBO_phys( zwx ):\n",
    "\n",
    "    # z     = tf.reshape( zDist.sample( ), [dimZ])\n",
    "    # w     = tf.reshape( wDist.sample( ), [dimW])\n",
    "    # # fixed grid\n",
    "    # # xrand = XTr\n",
    "\n",
    "    # # random uniform\n",
    "    # xrand = xDist.sample(sample_shape=[nxSamples, dimX])\n",
    "    # print(\"xrand.shape\", xrand.shape)\n",
    "\n",
    "    z,w,xrand = zwx[:dimZ], zwx[dimZ:dimZ+dimW], zwx[dimZ+dimW:]\n",
    "    xrand     = tf.reshape(xrand, [-1,1])\n",
    "    print(\"ELBO xrand.shape\", xrand.shape)\n",
    "    print('ELBO z.shape', z.shape)\n",
    "    print('ELBO w.shape', w.shape)\n",
    "\n",
    "    u_untrans, us, residual, logP_alpha_u_Z = residualFunction( z, w, xrand )\n",
    "\n",
    "    logVarEpsVect  = tf.repeat(2.*tf.math.log(epsilon_r), residual.shape[0] )[:, None]\n",
    "\n",
    "    logP_r   = log_normal_pdf( residual, tf.constant(0., shape=residual.shape), logVarEpsVect, raxis=0 )\n",
    "\n",
    "    mean_beta, logvar_beta = betaNet( tf.reshape(u_untrans, [-1,1]), w, xrand )\n",
    "\n",
    "    print('mean_beta.shape', mean_beta.shape)\n",
    "    print('mean_beta.shape', logvar_beta.shape)\n",
    "\n",
    "    logP_beta_Z_u          = log_normal_pdf( tf.reshape(z, [1,-1]), mean_beta, logvar_beta, raxis=1 ) #this is correct dimension\n",
    "\n",
    "    print('logP_alpha_u_Z.shape', logP_alpha_u_Z.shape)\n",
    "    print('logP_beta_Z_u.shape', logP_beta_Z_u.shape)\n",
    "    print('logP_r.shape', logP_r.shape)\n",
    "\n",
    "    elbo = tf.reduce_mean( logP_r + logP_beta_Z_u -  logP_alpha_u_Z )\n",
    "\n",
    "    return elbo\n",
    "\n",
    "zs =  zDist.sample( )\n",
    "ws =  wDist.sample( )\n",
    "xs =  xDist.sample( sample_shape=(nxSamples) )\n",
    "\n",
    "zwx = tf.concat([zs, ws, xs], 0)\n",
    "\n",
    "ELBO_phys( zwx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO_data( YZW ):\n",
    "\n",
    "    y, z, w = YZW[:dimY], YZW[dimY:dimY+dimZ], YZW[dimY+dimZ:]\n",
    "    y       = tf.reshape(y, [-1,1])\n",
    "\n",
    "    print('y.shape', y.shape)\n",
    "    print('z.shape', z.shape)\n",
    "    print('w.shape', w.shape)\n",
    "\n",
    "\n",
    "    mean_alpha, logvar_alpha = alphaNet( z, w, data_single_X )\n",
    "\n",
    "    Dvect = D( data_single_X )\n",
    "\n",
    "    mean_alpha_trans = Dvect * mean_alpha\n",
    "\n",
    "    var_trans_y  = Dvect ** 2. * tf.exp( logvar_alpha ) + sigma_y**2.\n",
    "\n",
    "    log_var_trans_y = tf.math.log( var_trans_y )\n",
    "\n",
    "    logP_Y_zwx       = log_normal_pdf(y, mean_alpha_trans, log_var_trans_y, raxis=0)\n",
    "\n",
    "    return tf.reshape( logP_Y_zwx, [ ] )\n",
    "\n",
    "ELBO_data(dataset_data_YZWX_stack[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainableVars   = [ alphaNet.trainable_variables, betaNet.trainable_variables]\n",
    "\n",
    "@tf.function\n",
    "def train_step(optimizer, batch_data):\n",
    "\n",
    "    zs =  zDist.sample( batchSize )\n",
    "    ws =  wDist.sample( batchSize )\n",
    "    xs =  xDist.sample( sample_shape=(batchSize, nxSamples) )\n",
    "\n",
    "    print('zs.shape', zs.shape)\n",
    "    print('ws.shape', ws.shape)\n",
    "    print('xs.shape', xs.shape)\n",
    "\n",
    "    zwxs = tf.concat([zs, ws, xs], 1)\n",
    "\n",
    "    print('zwxs.shape', zwxs.shape)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # posloss, logP_r, logP_beta_Z_u, logP_alpha_u_Z = ELBO( )\n",
    "        # loss    = - posloss\n",
    "\n",
    "        losses_phys =   tf.vectorized_map(ELBO_phys, zwxs)\n",
    "        loss_phys   = - tf.reduce_mean(losses_phys)\n",
    "\n",
    "        losses_data = tf.vectorized_map( ELBO_data, batch_data )\n",
    "        loss_data   = - nData_y / batch_size_data * tf.reduce_sum(losses_data)\n",
    "\n",
    "        print('loss_data = ', loss_data)\n",
    "\n",
    "        loss = loss_phys + loss_data\n",
    "\n",
    "    for vars in trainableVars:\n",
    "        gradients = tape.gradient(loss, vars)\n",
    "        optimizer.apply_gradients(zip(gradients, vars))\n",
    "\n",
    "    return loss, loss_data, loss_phys\n",
    "\n",
    "batchSize = 50\n",
    "\n",
    "num_iterations   = tf.constant(1_000_000)\n",
    "epochs           = num_iterations/int(nData_y/batch_size_data)/batchSize\n",
    "\n",
    "epsilon_r.assign( tf.constant(1e-2) )\n",
    "\n",
    "intervalELBOsave = 10\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    1e-3, int(num_iterations/batchSize/5), 0.5, staircase=True, name=None\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam( lr )\n",
    "\n",
    "elboAll     = []\n",
    "startTrainingPDDLVM = time.time()\n",
    "iteration   = tf.constant(0)\n",
    "for epoch in tf.range(0, epochs ):\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch_data in dataset_data_YZWX_batched:\n",
    "        loss, loss_data, loss_phys = train_step(optimizer, batch_data)\n",
    "        iteration += batchSize\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if epoch % intervalELBOsave == 0:\n",
    "        elboAll.append( -loss ) \n",
    "        print('Iteration: {}, Epoch: {}, ELBO: {:.2f}, loss_phys: {:.2f}, loss_data: {:.2f}, time elapse for current epoch: {:.2f}'\n",
    "           .format(iteration, epoch, -loss, -loss_phys, -loss_data, end_time - start_time))\n",
    "\n",
    "endTrainingPDDLVM = time.time()\n",
    "print('totalTimeTrainingPDLVM = {:.2f}m'.format((endTrainingPDDLVM - startTrainingPDDLVM)/60.))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebloAll = tf.stack(elboAll)\n",
    "LOSS    = np.array(ebloAll)\n",
    "plt.plot(range(LOSS.shape[0]), LOSS )\n",
    "plt.show()\n",
    "print(np.isnan(LOSS).mean())\n",
    "plt.plot(range(LOSS.shape[0]), np.log(-LOSS) )\n",
    "\n",
    "print('totalTimeTrainingPDLVM = ', (endTrainingPDDLVM - startTrainingPDDLVM)/60 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirnameGT = 'data-groundTruth--PoissonNonLinear1D'\n",
    "# dirnameGT = 'data-groundTruth--PoissonNonLinear1D_1000'\n",
    "# dirnameGT = '../../fenics/exp/poisson1DNonLinear-Comparisons/data-groundTruth--PoissonNonLinear1D_1000'\n",
    "dirnameGT = '../../../fenics/poisson1DNonLinear/data-groundTruth--PoissonNonLinear1D_1000'\n",
    "\n",
    "groundTruthU = tf.constant( np.loadtxt(dirnameGT+'/' + 'allGroundTruthU.dat' ), dtype=tf.float32)\n",
    "allTrueZs    = tf.constant( np.loadtxt(dirnameGT+'/' + 'allGroundTruthZ.dat' ), dtype=tf.float32)\n",
    "allTrueWs    = tf.constant( np.loadtxt(dirnameGT+'/' + 'allGroundTruthW.dat' ), dtype=tf.float32)\n",
    "groundTruthX = tf.constant( np.loadtxt(dirnameGT+'/' + 'FenicsMeshX.dat'     ), dtype=tf.float32)\n",
    "\n",
    "saveDataList = True\n",
    "\n",
    "if saveDataList:\n",
    "\n",
    "     allresAbsMean = []\n",
    "\n",
    "     allSampleZs   = []\n",
    "     allMeanZs     = []\n",
    "     allStdZs      = []\n",
    "\n",
    "     allUSamples = []\n",
    "     allUMeans   = []\n",
    "     allUStd     = []\n",
    "\n",
    "     allZNSE       = []\n",
    "     allUNSE       = []\n",
    "\n",
    "     Pin2SigK = []\n",
    "     Pin2SigU = []\n",
    "\n",
    "Xtest   = groundTruthX[:, None]\n",
    "print('Xtest.shape = ', Xtest.shape)\n",
    "\n",
    "tf_residualFunction = tf.function( residualFunction )\n",
    "tf_alphaNet         = tf.function( alphaNet )\n",
    "tf_betaNet          = tf.function( betaNet )\n",
    "\n",
    "PLOT = False\n",
    "\n",
    "for i in range(groundTruthU.shape[0]):\n",
    "# for i in [2,3,4,7,8]:\n",
    "\n",
    "     zTrue = allTrueZs[i]\n",
    "     wTrue = allTrueWs[i]\n",
    "     uTrue = groundTruthU[i]\n",
    "     \n",
    "     u_untrans, us, residual, logP_alpha_u_Z = tf_residualFunction( zTrue, wTrue, Xtest )\n",
    "     mean_alpha, logvar_alpha = tf_alphaNet( zTrue, wTrue, Xtest )\n",
    "     u_untrans = tf.reshape(reparameterize(mean_alpha, logvar_alpha), [-1,1])\n",
    "\n",
    "     u = tf.squeeze( D( Xtest ) * u_untrans )\n",
    "     mean_alpha_trans   = tf.squeeze( D( Xtest ) * mean_alpha )\n",
    "     var_alpha_trans    = tf.squeeze( D( Xtest )**2. * tf.exp(logvar_alpha) )\n",
    "     std_alpha_trans    = tf.sqrt( var_alpha_trans )\n",
    "     # std_alpha_trans    = tf.squeeze( D( Xtest )**2. * tf.exp(0.5 * logvar_alpha) )\n",
    "\n",
    "     if PLOT:\n",
    "          c = next(plt.gca()._get_lines.prop_cycler)['color']\n",
    "\n",
    "          plt.scatter(Xtest, u,                1,  color=c)#linewidth=1\n",
    "          plt.plot(Xtest, mean_alpha_trans,  '--', linewidth=1,  c=c)\n",
    "          plt.plot(Xtest, uTrue,   linewidth=1,    c=c)\n",
    "          plt.fill_between(Xtest[:,0], mean_alpha_trans - 2*std_alpha_trans, mean_alpha_trans + 2*std_alpha_trans,color=c, alpha=0.2)\n",
    "     \n",
    "     \n",
    "     mean_beta, logvar_beta = tf_betaNet( tf.reshape(uTrue/D(tf.squeeze(Xtest)), [-1,1]), wTrue,  Xtest )\n",
    "     mean_beta, logvar_beta = tf.squeeze( mean_beta), tf.squeeze( logvar_beta )\n",
    "     sampleZ  = reparameterize( mean_beta, logvar_beta )\n",
    "     std_beta = tf.exp( 0.5 * logvar_beta)\n",
    "\n",
    "     print('mean_beta = ', mean_beta)\n",
    "     print('std_beta = ' , tf.exp( 0.5 * logvar_beta))\n",
    "     print('true z = '   , zTrue)\n",
    "     print('\\n\\n')\n",
    "\n",
    "\n",
    "     if saveDataList:\n",
    "\n",
    "          allresAbsMean.append( tf.reduce_mean( tf.abs( residual[1:-1] ) ) )\n",
    "\n",
    "          allSampleZs.append(tf.squeeze(sampleZ))\n",
    "          allMeanZs.append(tf.squeeze(mean_beta))\n",
    "          allStdZs.append(tf.squeeze(tf.exp( 0.5 * logvar_beta)))\n",
    "\n",
    "          allUSamples.append(tf.squeeze(us))\n",
    "          allUMeans.append(tf.squeeze(mean_alpha_trans))\n",
    "          allUStd.append(tf.squeeze(mean_alpha_trans))\n",
    "          \n",
    "          allZNSE.append( ( tf.linalg.norm((tf.squeeze(mean_beta) - tf.squeeze(zTrue)) ) / tf.linalg.norm(tf.squeeze(zTrue)) )**2. )\n",
    "\n",
    "          allUNSE.append( ( tf.linalg.norm((tf.squeeze(mean_alpha_trans) - tf.squeeze(uTrue)) ) / tf.linalg.norm(tf.squeeze(uTrue)) )**2. )\n",
    "\n",
    "\n",
    "          Pin2SigK.append( tf.reduce_sum(tf.where(tf.math.logical_and( \n",
    "               zTrue > mean_beta-2*std_beta, zTrue < mean_beta+2*std_beta\n",
    "               ), 1, 0)) / zTrue.shape[0] )\n",
    "\n",
    "          Pin2SigU.append( tf.reduce_sum(tf.where(tf.math.logical_and(\n",
    "               uTrue > mean_alpha_trans-2*std_alpha_trans, uTrue < mean_alpha_trans+2*std_alpha_trans\n",
    "               ), 1, 0)) / uTrue.shape[0]  )\n",
    "\n",
    "\n",
    "# import matplotlib.lines as mlines\n",
    "# from matplotlib.patches import Patch\n",
    "# samples     = mlines.Line2D([], [],  color='blue', linestyle=':',  linewidth=1., label=r'$samples$')\n",
    "# dashed      = mlines.Line2D([], [],  color='blue', linestyle='--', linewidth=1., label=r'$mean$')\n",
    "# solid       = mlines.Line2D([], [],  color='blue', linestyle='-', linewidth=1.,    label=r'$truth$')\n",
    "# fill        = mlines.Line2D([], [],  color='blue', linestyle='-', linewidth=5.,    label=r'$2\\sigma$')\n",
    "# fill        = Patch(facecolor='blue', edgecolor='blue', label='Color Patch')\n",
    "# plt.legend(handles=[samples, dashed, solid, fill], loc='best') #best upper-right\n",
    "if PLOT:\n",
    "     plt.xlabel(r'$x$')\n",
    "     plt.ylabel(r'$u(x)\\pm2\\sigma$')\n",
    "     plt.tight_layout()\n",
    "     # plt.savefig(dirname + 'samplePlotsTrusVdiaglfm.pdf')\n",
    "     plt.show()\n",
    "\n",
    "# allresAbsMeanStacked = tf.stack( allresAbsMean )\n",
    "\n",
    "# print('mean allresAbsMeanStacked = ', tf.reduce_mean( allresAbsMeanStacked ) )\n",
    "\n",
    "print('mean allZNSE = ', tf.reduce_mean( tf.stack(allZNSE) ) )\n",
    "print('mean allUNSE = ', tf.reduce_mean( tf.stack(allUNSE) ) )\n",
    "\n",
    "print('sttdev allZNSE = ', tf.math.reduce_std( tf.stack(allZNSE) ) )\n",
    "print('sttdev allUNSE = ', tf.math.reduce_std( tf.stack(allUNSE) ) )\n",
    "\n",
    "# print('median allZNSE = ', tfp.stats.percentile( allZNSE, 50 ) )\n",
    "\n",
    "print('Pin2SigK = ', tf.reduce_mean(tf.stack(Pin2SigK)))\n",
    "print('Pin2SigU = ', tf.reduce_mean(tf.stack(Pin2SigU)))\n",
    "print('totalTimeTrainingPDLVM = ', (endTrainingPDDLVM - startTrainingPDDLVM)/60 )\n",
    "\n",
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    alphaNet.save_weights(dirname + '/alphaModel/alphaModel-lfmDiag')\n",
    "    # alphaNet.load_weights(dirname + '/alphaModel/alphaModel-lfmDiag')\n",
    "\n",
    "    # del alphaNet\n",
    "\n",
    "if True:\n",
    "    betaNet.save_weights(dirname + '/betaModel/betaModel-lfmDiag')\n",
    "    # betaNet.load_weights(dirname + '/betaModel/betaModel-lfmDiag')\n",
    "\n",
    "    # del betaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = True\n",
    "if saveData:\n",
    "\n",
    "    allSampleZs_np = tf.stack(allSampleZs)\n",
    "    allMeanZs_np   = tf.stack(allMeanZs)\n",
    "    allStdZs_np    = tf.stack(allStdZs)\n",
    "\n",
    "    allUSamples_np  = tf.stack(allUSamples)\n",
    "    allUMeans_np    = tf.stack(allUMeans)\n",
    "    allUStd_np      = tf.stack(allUStd)\n",
    "\n",
    "    prefix = 'randgrid{}_{}kIter'.format(nxSamples, int(num_iterations/1000))\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allRes_np.dat'   , tf.stack(tf.squeeze(allresAbsMean)) )\n",
    "\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allSampleZs_np.dat', allSampleZs_np)\n",
    "    np.savetxt(dirname + prefix + '_allMeanZs_np.dat'  , allMeanZs_np)\n",
    "    np.savetxt(dirname + prefix + '_allStdZs_np.dat'  , allMeanZs_np)\n",
    "\n",
    "\n",
    "    np.savetxt(dirname + prefix + '_allUSamples_np.dat' , allUSamples_np)\n",
    "    np.savetxt(dirname + prefix + '_allUMeans_np.dat'   , allUMeans_np)\n",
    "    np.savetxt(dirname + prefix + '_allUStd_np.dat'     , allUStd_np)\n",
    "    \n",
    "    np.savetxt(dirname + prefix + '_allZNSE_np.dat'   , tf.stack( allZNSE ) )\n",
    "    np.savetxt(dirname + prefix + '_allUNSE_np.dat'   , tf.stack( allUNSE ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in betaNet.NN.kipLayers:\n",
    "    print(layer.ell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('lfm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "56d2bb5d7cc3676b4f24fb92af7a44f3b6495c3f95e7930a39b9feb733cbd04c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
